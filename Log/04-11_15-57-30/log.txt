Training: Epoch[001/035] Iteration[010/381] Loss: 6.6207 Acc:0.00%
Training: Epoch[001/035] Iteration[020/381] Loss: 6.6210 Acc:0.16%
Training: Epoch[001/035] Iteration[030/381] Loss: 6.6154 Acc:0.21%
Training: Epoch[001/035] Iteration[040/381] Loss: 6.6115 Acc:0.16%
Training: Epoch[001/035] Iteration[050/381] Loss: 6.6068 Acc:0.38%
Training: Epoch[001/035] Iteration[060/381] Loss: 6.6043 Acc:0.42%
Training: Epoch[001/035] Iteration[070/381] Loss: 6.6001 Acc:0.62%
Training: Epoch[001/035] Iteration[080/381] Loss: 6.5939 Acc:0.98%
Training: Epoch[001/035] Iteration[090/381] Loss: 6.5882 Acc:1.28%
Training: Epoch[001/035] Iteration[100/381] Loss: 6.5774 Acc:1.66%
Training: Epoch[001/035] Iteration[110/381] Loss: 6.5790 Acc:1.76%
Training: Epoch[001/035] Iteration[120/381] Loss: 6.5725 Acc:2.06%
Training: Epoch[001/035] Iteration[130/381] Loss: 6.5680 Acc:2.36%
Training: Epoch[001/035] Iteration[140/381] Loss: 6.5744 Acc:2.48%
Training: Epoch[001/035] Iteration[150/381] Loss: 6.5525 Acc:2.98%
Training: Epoch[001/035] Iteration[160/381] Loss: 6.5568 Acc:3.07%
Training: Epoch[001/035] Iteration[170/381] Loss: 6.5542 Acc:3.22%
Training: Epoch[001/035] Iteration[180/381] Loss: 6.5436 Acc:3.39%
Training: Epoch[001/035] Iteration[190/381] Loss: 6.5390 Acc:3.62%
Training: Epoch[001/035] Iteration[200/381] Loss: 6.5236 Acc:3.88%
Training: Epoch[001/035] Iteration[210/381] Loss: 6.5274 Acc:4.06%
Training: Epoch[001/035] Iteration[220/381] Loss: 6.5290 Acc:4.16%
Training: Epoch[001/035] Iteration[230/381] Loss: 6.5097 Acc:4.35%
Training: Epoch[001/035] Iteration[240/381] Loss: 6.4970 Acc:4.56%
Training: Epoch[001/035] Iteration[250/381] Loss: 6.5028 Acc:4.70%
Training: Epoch[001/035] Iteration[260/381] Loss: 6.4932 Acc:4.82%
Training: Epoch[001/035] Iteration[270/381] Loss: 6.4821 Acc:5.01%
Training: Epoch[001/035] Iteration[280/381] Loss: 6.4771 Acc:5.16%
Training: Epoch[001/035] Iteration[290/381] Loss: 6.4831 Acc:5.33%
Training: Epoch[001/035] Iteration[300/381] Loss: 6.4659 Acc:5.38%
Training: Epoch[001/035] Iteration[310/381] Loss: 6.4467 Acc:5.50%
Training: Epoch[001/035] Iteration[320/381] Loss: 6.4488 Acc:5.55%
Training: Epoch[001/035] Iteration[330/381] Loss: 6.4317 Acc:5.67%
Training: Epoch[001/035] Iteration[340/381] Loss: 6.4301 Acc:5.74%
Training: Epoch[001/035] Iteration[350/381] Loss: 6.4265 Acc:5.83%
Training: Epoch[001/035] Iteration[360/381] Loss: 6.4132 Acc:5.95%
Training: Epoch[001/035] Iteration[370/381] Loss: 6.4007 Acc:6.06%
Training: Epoch[001/035] Iteration[380/381] Loss: 6.3925 Acc:6.13%
Valid set Accuracy:4.53%
Training: Epoch[002/035] Iteration[010/381] Loss: 6.2344 Acc:13.12%
Training: Epoch[002/035] Iteration[020/381] Loss: 5.9816 Acc:8.75%
Training: Epoch[002/035] Iteration[030/381] Loss: 5.6462 Acc:8.65%
Training: Epoch[002/035] Iteration[040/381] Loss: 5.1838 Acc:8.83%
Training: Epoch[002/035] Iteration[050/381] Loss: 5.1070 Acc:9.19%
Training: Epoch[002/035] Iteration[060/381] Loss: 4.6931 Acc:10.31%
Training: Epoch[002/035] Iteration[070/381] Loss: 4.4471 Acc:11.65%
Training: Epoch[002/035] Iteration[080/381] Loss: 3.9833 Acc:13.20%
Training: Epoch[002/035] Iteration[090/381] Loss: 3.9965 Acc:14.27%
Training: Epoch[002/035] Iteration[100/381] Loss: 3.9527 Acc:15.06%
Training: Epoch[002/035] Iteration[110/381] Loss: 3.5599 Acc:16.25%
Training: Epoch[002/035] Iteration[120/381] Loss: 3.2783 Acc:17.76%
Training: Epoch[002/035] Iteration[130/381] Loss: 3.4649 Acc:18.63%
Training: Epoch[002/035] Iteration[140/381] Loss: 3.2371 Acc:19.58%
Training: Epoch[002/035] Iteration[150/381] Loss: 2.8789 Acc:20.77%
Training: Epoch[002/035] Iteration[160/381] Loss: 2.9099 Acc:21.93%
Training: Epoch[002/035] Iteration[170/381] Loss: 2.8930 Acc:23.09%
Training: Epoch[002/035] Iteration[180/381] Loss: 2.9630 Acc:24.06%
Training: Epoch[002/035] Iteration[190/381] Loss: 2.8115 Acc:24.93%
Training: Epoch[002/035] Iteration[200/381] Loss: 2.4649 Acc:25.95%
Training: Epoch[002/035] Iteration[210/381] Loss: 2.3373 Acc:27.04%
Training: Epoch[002/035] Iteration[220/381] Loss: 2.4914 Acc:27.84%
Training: Epoch[002/035] Iteration[230/381] Loss: 2.3410 Acc:28.80%
Training: Epoch[002/035] Iteration[240/381] Loss: 2.2629 Acc:29.61%
Training: Epoch[002/035] Iteration[250/381] Loss: 2.3006 Acc:30.41%
Training: Epoch[002/035] Iteration[260/381] Loss: 2.0987 Acc:31.33%
Training: Epoch[002/035] Iteration[270/381] Loss: 2.2076 Acc:31.97%
Training: Epoch[002/035] Iteration[280/381] Loss: 2.0994 Acc:32.67%
Training: Epoch[002/035] Iteration[290/381] Loss: 1.8133 Acc:33.50%
Training: Epoch[002/035] Iteration[300/381] Loss: 1.8129 Acc:34.25%
Training: Epoch[002/035] Iteration[310/381] Loss: 1.8155 Acc:34.96%
Training: Epoch[002/035] Iteration[320/381] Loss: 1.9030 Acc:35.72%
Training: Epoch[002/035] Iteration[330/381] Loss: 1.7502 Acc:36.43%
Training: Epoch[002/035] Iteration[340/381] Loss: 1.7768 Acc:36.99%
Training: Epoch[002/035] Iteration[350/381] Loss: 1.6231 Acc:37.67%
Training: Epoch[002/035] Iteration[360/381] Loss: 1.6505 Acc:38.27%
Training: Epoch[002/035] Iteration[370/381] Loss: 1.6909 Acc:38.83%
Training: Epoch[002/035] Iteration[380/381] Loss: 1.6142 Acc:39.39%
Training: Epoch[003/035] Iteration[010/381] Loss: 1.0366 Acc:71.25%
Training: Epoch[003/035] Iteration[020/381] Loss: 1.0603 Acc:72.34%
Training: Epoch[003/035] Iteration[030/381] Loss: 1.0165 Acc:71.67%
Training: Epoch[003/035] Iteration[040/381] Loss: 1.1152 Acc:71.48%
Training: Epoch[003/035] Iteration[050/381] Loss: 1.0585 Acc:71.50%
Training: Epoch[003/035] Iteration[060/381] Loss: 1.0963 Acc:71.25%
Training: Epoch[003/035] Iteration[070/381] Loss: 1.2359 Acc:71.21%
Training: Epoch[003/035] Iteration[080/381] Loss: 1.0534 Acc:71.29%
Training: Epoch[003/035] Iteration[090/381] Loss: 0.9672 Acc:71.53%
Training: Epoch[003/035] Iteration[100/381] Loss: 1.1263 Acc:71.34%
Training: Epoch[003/035] Iteration[110/381] Loss: 1.1324 Acc:71.31%
Training: Epoch[003/035] Iteration[120/381] Loss: 0.9632 Acc:71.59%
Training: Epoch[003/035] Iteration[130/381] Loss: 0.9072 Acc:71.97%
Training: Epoch[003/035] Iteration[140/381] Loss: 0.9501 Acc:72.17%
Training: Epoch[003/035] Iteration[150/381] Loss: 0.9689 Acc:72.40%
Training: Epoch[003/035] Iteration[160/381] Loss: 1.0006 Acc:72.44%
Training: Epoch[003/035] Iteration[170/381] Loss: 0.9166 Acc:72.74%
Training: Epoch[003/035] Iteration[180/381] Loss: 0.7678 Acc:73.16%
Training: Epoch[003/035] Iteration[190/381] Loss: 0.8635 Acc:73.49%
Training: Epoch[003/035] Iteration[200/381] Loss: 0.8810 Acc:73.73%
Training: Epoch[003/035] Iteration[210/381] Loss: 0.8934 Acc:73.79%
Training: Epoch[003/035] Iteration[220/381] Loss: 0.8538 Acc:74.06%
Training: Epoch[003/035] Iteration[230/381] Loss: 0.8578 Acc:74.25%
Training: Epoch[003/035] Iteration[240/381] Loss: 0.9677 Acc:74.21%
Training: Epoch[003/035] Iteration[250/381] Loss: 0.8469 Acc:74.35%
Training: Epoch[003/035] Iteration[260/381] Loss: 0.7479 Acc:74.60%
Training: Epoch[003/035] Iteration[270/381] Loss: 0.9559 Acc:74.68%
Training: Epoch[003/035] Iteration[280/381] Loss: 0.8250 Acc:74.84%
Training: Epoch[003/035] Iteration[290/381] Loss: 0.9271 Acc:74.88%
Training: Epoch[003/035] Iteration[300/381] Loss: 0.6807 Acc:75.07%
Training: Epoch[003/035] Iteration[310/381] Loss: 0.8570 Acc:75.13%
Training: Epoch[003/035] Iteration[320/381] Loss: 0.8467 Acc:75.22%
Training: Epoch[003/035] Iteration[330/381] Loss: 0.7097 Acc:75.36%
Training: Epoch[003/035] Iteration[340/381] Loss: 0.7274 Acc:75.47%
Training: Epoch[003/035] Iteration[350/381] Loss: 0.9482 Acc:75.41%
Training: Epoch[003/035] Iteration[360/381] Loss: 0.7764 Acc:75.43%
Training: Epoch[003/035] Iteration[370/381] Loss: 0.6441 Acc:75.59%
Training: Epoch[003/035] Iteration[380/381] Loss: 0.7321 Acc:75.67%
Valid set Accuracy:72.30%
Training: Epoch[004/035] Iteration[010/381] Loss: 0.5295 Acc:84.38%
Training: Epoch[004/035] Iteration[020/381] Loss: 0.4863 Acc:84.84%
Training: Epoch[004/035] Iteration[030/381] Loss: 0.3241 Acc:86.98%
Training: Epoch[004/035] Iteration[040/381] Loss: 0.3013 Acc:87.81%
Training: Epoch[004/035] Iteration[050/381] Loss: 0.3902 Acc:87.94%
Training: Epoch[004/035] Iteration[060/381] Loss: 0.5727 Acc:87.40%
Training: Epoch[004/035] Iteration[070/381] Loss: 0.5501 Acc:86.88%
Training: Epoch[004/035] Iteration[080/381] Loss: 0.4098 Acc:86.95%
Training: Epoch[004/035] Iteration[090/381] Loss: 0.3958 Acc:87.01%
Training: Epoch[004/035] Iteration[100/381] Loss: 0.4387 Acc:87.03%
Training: Epoch[004/035] Iteration[110/381] Loss: 0.4866 Acc:87.10%
Training: Epoch[004/035] Iteration[120/381] Loss: 0.4649 Acc:87.08%
Training: Epoch[004/035] Iteration[130/381] Loss: 0.4556 Acc:87.16%
Training: Epoch[004/035] Iteration[140/381] Loss: 0.5248 Acc:87.05%
Training: Epoch[004/035] Iteration[150/381] Loss: 0.5007 Acc:87.00%
Training: Epoch[004/035] Iteration[160/381] Loss: 0.5971 Acc:86.82%
Training: Epoch[004/035] Iteration[170/381] Loss: 0.4874 Acc:86.88%
Training: Epoch[004/035] Iteration[180/381] Loss: 0.4268 Acc:86.94%
Training: Epoch[004/035] Iteration[190/381] Loss: 0.3867 Acc:87.07%
Training: Epoch[004/035] Iteration[200/381] Loss: 0.4286 Acc:87.09%
Training: Epoch[004/035] Iteration[210/381] Loss: 0.3674 Acc:87.17%
Training: Epoch[004/035] Iteration[220/381] Loss: 0.5203 Acc:87.14%
Training: Epoch[004/035] Iteration[230/381] Loss: 0.5055 Acc:87.00%
Training: Epoch[004/035] Iteration[240/381] Loss: 0.4453 Acc:87.07%
Training: Epoch[004/035] Iteration[250/381] Loss: 0.5888 Acc:86.95%
Training: Epoch[004/035] Iteration[260/381] Loss: 0.3499 Acc:87.03%
Training: Epoch[004/035] Iteration[270/381] Loss: 0.3930 Acc:87.13%
Training: Epoch[004/035] Iteration[280/381] Loss: 0.4849 Acc:87.12%
Training: Epoch[004/035] Iteration[290/381] Loss: 0.4408 Acc:87.12%
Training: Epoch[004/035] Iteration[300/381] Loss: 0.5025 Acc:87.11%
Training: Epoch[004/035] Iteration[310/381] Loss: 0.3606 Acc:87.22%
Training: Epoch[004/035] Iteration[320/381] Loss: 0.4923 Acc:87.19%
Training: Epoch[004/035] Iteration[330/381] Loss: 0.4706 Acc:87.15%
Training: Epoch[004/035] Iteration[340/381] Loss: 0.4564 Acc:87.17%
Training: Epoch[004/035] Iteration[350/381] Loss: 0.5146 Acc:87.16%
Training: Epoch[004/035] Iteration[360/381] Loss: 0.3530 Acc:87.25%
Training: Epoch[004/035] Iteration[370/381] Loss: 0.4362 Acc:87.20%
Training: Epoch[004/035] Iteration[380/381] Loss: 0.4079 Acc:87.19%
Training: Epoch[005/035] Iteration[010/381] Loss: 0.3770 Acc:88.44%
Training: Epoch[005/035] Iteration[020/381] Loss: 0.2800 Acc:90.16%
Training: Epoch[005/035] Iteration[030/381] Loss: 0.2734 Acc:90.62%
Training: Epoch[005/035] Iteration[040/381] Loss: 0.2971 Acc:90.86%
Training: Epoch[005/035] Iteration[050/381] Loss: 0.2890 Acc:90.56%
Training: Epoch[005/035] Iteration[060/381] Loss: 0.2688 Acc:91.15%
Training: Epoch[005/035] Iteration[070/381] Loss: 0.2407 Acc:91.43%
Training: Epoch[005/035] Iteration[080/381] Loss: 0.2342 Acc:91.45%
Training: Epoch[005/035] Iteration[090/381] Loss: 0.2617 Acc:91.49%
Training: Epoch[005/035] Iteration[100/381] Loss: 0.3336 Acc:91.53%
Training: Epoch[005/035] Iteration[110/381] Loss: 0.2844 Acc:91.53%
Training: Epoch[005/035] Iteration[120/381] Loss: 0.2885 Acc:91.54%
Training: Epoch[005/035] Iteration[130/381] Loss: 0.2014 Acc:91.85%
Training: Epoch[005/035] Iteration[140/381] Loss: 0.1735 Acc:92.03%
Training: Epoch[005/035] Iteration[150/381] Loss: 0.2348 Acc:92.15%
Training: Epoch[005/035] Iteration[160/381] Loss: 0.1497 Acc:92.32%
Training: Epoch[005/035] Iteration[170/381] Loss: 0.2378 Acc:92.32%
Training: Epoch[005/035] Iteration[180/381] Loss: 0.2586 Acc:92.29%
Training: Epoch[005/035] Iteration[190/381] Loss: 0.3421 Acc:92.24%
Training: Epoch[005/035] Iteration[200/381] Loss: 0.2285 Acc:92.30%
Training: Epoch[005/035] Iteration[210/381] Loss: 0.2225 Acc:92.37%
Training: Epoch[005/035] Iteration[220/381] Loss: 0.2195 Acc:92.47%
Training: Epoch[005/035] Iteration[230/381] Loss: 0.2136 Acc:92.53%
Training: Epoch[005/035] Iteration[240/381] Loss: 0.3342 Acc:92.40%
Training: Epoch[005/035] Iteration[250/381] Loss: 0.2459 Acc:92.45%
Training: Epoch[005/035] Iteration[260/381] Loss: 0.2904 Acc:92.39%
Training: Epoch[005/035] Iteration[270/381] Loss: 0.2395 Acc:92.44%
Training: Epoch[005/035] Iteration[280/381] Loss: 0.3211 Acc:92.29%
Training: Epoch[005/035] Iteration[290/381] Loss: 0.3428 Acc:92.26%
Training: Epoch[005/035] Iteration[300/381] Loss: 0.3903 Acc:92.15%
Training: Epoch[005/035] Iteration[310/381] Loss: 0.3421 Acc:92.08%
Training: Epoch[005/035] Iteration[320/381] Loss: 0.2385 Acc:92.15%
Training: Epoch[005/035] Iteration[330/381] Loss: 0.1896 Acc:92.20%
Training: Epoch[005/035] Iteration[340/381] Loss: 0.3274 Acc:92.13%
Training: Epoch[005/035] Iteration[350/381] Loss: 0.2307 Acc:92.16%
Training: Epoch[005/035] Iteration[360/381] Loss: 0.2700 Acc:92.12%
Training: Epoch[005/035] Iteration[370/381] Loss: 0.2617 Acc:92.15%
Training: Epoch[005/035] Iteration[380/381] Loss: 0.2753 Acc:92.18%
Valid set Accuracy:80.56%
Training: Epoch[006/035] Iteration[010/381] Loss: 0.1205 Acc:97.50%
Training: Epoch[006/035] Iteration[020/381] Loss: 0.1921 Acc:96.56%
Training: Epoch[006/035] Iteration[030/381] Loss: 0.1747 Acc:95.94%
Training: Epoch[006/035] Iteration[040/381] Loss: 0.1424 Acc:96.02%
Training: Epoch[006/035] Iteration[050/381] Loss: 0.1836 Acc:95.69%
Training: Epoch[006/035] Iteration[060/381] Loss: 0.1029 Acc:95.89%
Training: Epoch[006/035] Iteration[070/381] Loss: 0.1881 Acc:95.71%
Training: Epoch[006/035] Iteration[080/381] Loss: 0.2015 Acc:95.43%
Training: Epoch[006/035] Iteration[090/381] Loss: 0.2269 Acc:95.24%
Training: Epoch[006/035] Iteration[100/381] Loss: 0.1315 Acc:95.28%
Training: Epoch[006/035] Iteration[110/381] Loss: 0.1515 Acc:95.31%
Training: Epoch[006/035] Iteration[120/381] Loss: 0.1206 Acc:95.49%
Training: Epoch[006/035] Iteration[130/381] Loss: 0.1170 Acc:95.58%
Training: Epoch[006/035] Iteration[140/381] Loss: 0.1618 Acc:95.56%
Training: Epoch[006/035] Iteration[150/381] Loss: 0.2174 Acc:95.42%
Training: Epoch[006/035] Iteration[160/381] Loss: 0.1538 Acc:95.41%
Training: Epoch[006/035] Iteration[170/381] Loss: 0.1487 Acc:95.44%
Training: Epoch[006/035] Iteration[180/381] Loss: 0.1751 Acc:95.40%
Training: Epoch[006/035] Iteration[190/381] Loss: 0.1818 Acc:95.35%
Training: Epoch[006/035] Iteration[200/381] Loss: 0.1680 Acc:95.36%
Training: Epoch[006/035] Iteration[210/381] Loss: 0.1724 Acc:95.33%
Training: Epoch[006/035] Iteration[220/381] Loss: 0.1518 Acc:95.37%
Training: Epoch[006/035] Iteration[230/381] Loss: 0.2654 Acc:95.24%
Training: Epoch[006/035] Iteration[240/381] Loss: 0.2861 Acc:95.09%
Training: Epoch[006/035] Iteration[250/381] Loss: 0.1263 Acc:95.17%
Training: Epoch[006/035] Iteration[260/381] Loss: 0.1518 Acc:95.18%
Training: Epoch[006/035] Iteration[270/381] Loss: 0.1381 Acc:95.25%
Training: Epoch[006/035] Iteration[280/381] Loss: 0.1136 Acc:95.32%
Training: Epoch[006/035] Iteration[290/381] Loss: 0.2338 Acc:95.30%
Training: Epoch[006/035] Iteration[300/381] Loss: 0.1883 Acc:95.27%
Training: Epoch[006/035] Iteration[310/381] Loss: 0.1565 Acc:95.29%
Training: Epoch[006/035] Iteration[320/381] Loss: 0.1257 Acc:95.34%
Training: Epoch[006/035] Iteration[330/381] Loss: 0.2068 Acc:95.33%
Training: Epoch[006/035] Iteration[340/381] Loss: 0.1992 Acc:95.35%
Training: Epoch[006/035] Iteration[350/381] Loss: 0.1623 Acc:95.36%
Training: Epoch[006/035] Iteration[360/381] Loss: 0.1680 Acc:95.36%
Training: Epoch[006/035] Iteration[370/381] Loss: 0.1999 Acc:95.34%
Training: Epoch[006/035] Iteration[380/381] Loss: 0.2500 Acc:95.30%
Training: Epoch[007/035] Iteration[010/381] Loss: 0.1421 Acc:96.88%
Training: Epoch[007/035] Iteration[020/381] Loss: 0.1655 Acc:96.25%
Training: Epoch[007/035] Iteration[030/381] Loss: 0.1218 Acc:96.35%
Training: Epoch[007/035] Iteration[040/381] Loss: 0.1130 Acc:96.48%
Training: Epoch[007/035] Iteration[050/381] Loss: 0.1570 Acc:96.31%
Training: Epoch[007/035] Iteration[060/381] Loss: 0.0706 Acc:96.51%
Training: Epoch[007/035] Iteration[070/381] Loss: 0.1054 Acc:96.52%
Training: Epoch[007/035] Iteration[080/381] Loss: 0.0997 Acc:96.64%
Training: Epoch[007/035] Iteration[090/381] Loss: 0.1548 Acc:96.42%
Training: Epoch[007/035] Iteration[100/381] Loss: 0.1227 Acc:96.38%
Training: Epoch[007/035] Iteration[110/381] Loss: 0.1156 Acc:96.42%
Training: Epoch[007/035] Iteration[120/381] Loss: 0.1739 Acc:96.38%
Training: Epoch[007/035] Iteration[130/381] Loss: 0.1078 Acc:96.39%
Training: Epoch[007/035] Iteration[140/381] Loss: 0.1249 Acc:96.34%
Training: Epoch[007/035] Iteration[150/381] Loss: 0.0955 Acc:96.42%
Training: Epoch[007/035] Iteration[160/381] Loss: 0.1274 Acc:96.35%
Training: Epoch[007/035] Iteration[170/381] Loss: 0.1568 Acc:96.25%
Training: Epoch[007/035] Iteration[180/381] Loss: 0.1101 Acc:96.28%
Training: Epoch[007/035] Iteration[190/381] Loss: 0.1546 Acc:96.25%
Training: Epoch[007/035] Iteration[200/381] Loss: 0.1567 Acc:96.25%
Training: Epoch[007/035] Iteration[210/381] Loss: 0.1636 Acc:96.22%
Training: Epoch[007/035] Iteration[220/381] Loss: 0.1049 Acc:96.28%
Training: Epoch[007/035] Iteration[230/381] Loss: 0.1156 Acc:96.28%
Training: Epoch[007/035] Iteration[240/381] Loss: 0.0658 Acc:96.34%
Training: Epoch[007/035] Iteration[250/381] Loss: 0.1926 Acc:96.34%
Training: Epoch[007/035] Iteration[260/381] Loss: 0.1239 Acc:96.31%
Training: Epoch[007/035] Iteration[270/381] Loss: 0.1181 Acc:96.35%
Training: Epoch[007/035] Iteration[280/381] Loss: 0.0844 Acc:96.42%
Training: Epoch[007/035] Iteration[290/381] Loss: 0.1169 Acc:96.39%
Training: Epoch[007/035] Iteration[300/381] Loss: 0.2035 Acc:96.38%
Training: Epoch[007/035] Iteration[310/381] Loss: 0.1174 Acc:96.39%
Training: Epoch[007/035] Iteration[320/381] Loss: 0.0940 Acc:96.44%
Training: Epoch[007/035] Iteration[330/381] Loss: 0.1157 Acc:96.44%
Training: Epoch[007/035] Iteration[340/381] Loss: 0.1727 Acc:96.44%
Training: Epoch[007/035] Iteration[350/381] Loss: 0.2079 Acc:96.43%
Training: Epoch[007/035] Iteration[360/381] Loss: 0.2085 Acc:96.37%
Training: Epoch[007/035] Iteration[370/381] Loss: 0.1613 Acc:96.34%
Training: Epoch[007/035] Iteration[380/381] Loss: 0.1036 Acc:96.33%
Valid set Accuracy:82.29%
Training: Epoch[008/035] Iteration[010/381] Loss: 0.1425 Acc:96.25%
Training: Epoch[008/035] Iteration[020/381] Loss: 0.0944 Acc:96.72%
Training: Epoch[008/035] Iteration[030/381] Loss: 0.1087 Acc:96.67%
Training: Epoch[008/035] Iteration[040/381] Loss: 0.1011 Acc:96.88%
Training: Epoch[008/035] Iteration[050/381] Loss: 0.0760 Acc:97.12%
Training: Epoch[008/035] Iteration[060/381] Loss: 0.0496 Acc:97.45%
Training: Epoch[008/035] Iteration[070/381] Loss: 0.0985 Acc:97.54%
Training: Epoch[008/035] Iteration[080/381] Loss: 0.0667 Acc:97.62%
Training: Epoch[008/035] Iteration[090/381] Loss: 0.0634 Acc:97.71%
Training: Epoch[008/035] Iteration[100/381] Loss: 0.0956 Acc:97.72%
Training: Epoch[008/035] Iteration[110/381] Loss: 0.0570 Acc:97.78%
Training: Epoch[008/035] Iteration[120/381] Loss: 0.0455 Acc:97.84%
Training: Epoch[008/035] Iteration[130/381] Loss: 0.0730 Acc:97.84%
Training: Epoch[008/035] Iteration[140/381] Loss: 0.0891 Acc:97.77%
Training: Epoch[008/035] Iteration[150/381] Loss: 0.1072 Acc:97.62%
Training: Epoch[008/035] Iteration[160/381] Loss: 0.1285 Acc:97.54%
Training: Epoch[008/035] Iteration[170/381] Loss: 0.1964 Acc:97.39%
Training: Epoch[008/035] Iteration[180/381] Loss: 0.1332 Acc:97.38%
Training: Epoch[008/035] Iteration[190/381] Loss: 0.0901 Acc:97.40%
Training: Epoch[008/035] Iteration[200/381] Loss: 0.1160 Acc:97.34%
Training: Epoch[008/035] Iteration[210/381] Loss: 0.1342 Acc:97.25%
Training: Epoch[008/035] Iteration[220/381] Loss: 0.0957 Acc:97.27%
Training: Epoch[008/035] Iteration[230/381] Loss: 0.0737 Acc:97.27%
Training: Epoch[008/035] Iteration[240/381] Loss: 0.0732 Acc:97.30%
Training: Epoch[008/035] Iteration[250/381] Loss: 0.0732 Acc:97.35%
Training: Epoch[008/035] Iteration[260/381] Loss: 0.0980 Acc:97.32%
Training: Epoch[008/035] Iteration[270/381] Loss: 0.1260 Acc:97.29%
Training: Epoch[008/035] Iteration[280/381] Loss: 0.0762 Acc:97.32%
Training: Epoch[008/035] Iteration[290/381] Loss: 0.1212 Acc:97.31%
Training: Epoch[008/035] Iteration[300/381] Loss: 0.1481 Acc:97.28%
Training: Epoch[008/035] Iteration[310/381] Loss: 0.1572 Acc:97.29%
Training: Epoch[008/035] Iteration[320/381] Loss: 0.0832 Acc:97.29%
Training: Epoch[008/035] Iteration[330/381] Loss: 0.0878 Acc:97.30%
Training: Epoch[008/035] Iteration[340/381] Loss: 0.1068 Acc:97.30%
Training: Epoch[008/035] Iteration[350/381] Loss: 0.0770 Acc:97.30%
Training: Epoch[008/035] Iteration[360/381] Loss: 0.1679 Acc:97.22%
Training: Epoch[008/035] Iteration[370/381] Loss: 0.1313 Acc:97.18%
Training: Epoch[008/035] Iteration[380/381] Loss: 0.1192 Acc:97.15%
Training: Epoch[009/035] Iteration[010/381] Loss: 0.0941 Acc:96.56%
Training: Epoch[009/035] Iteration[020/381] Loss: 0.0922 Acc:96.56%
Training: Epoch[009/035] Iteration[030/381] Loss: 0.0899 Acc:96.88%
Training: Epoch[009/035] Iteration[040/381] Loss: 0.1125 Acc:96.80%
Training: Epoch[009/035] Iteration[050/381] Loss: 0.0736 Acc:96.94%
Training: Epoch[009/035] Iteration[060/381] Loss: 0.0937 Acc:96.98%
Training: Epoch[009/035] Iteration[070/381] Loss: 0.0603 Acc:97.19%
Training: Epoch[009/035] Iteration[080/381] Loss: 0.0399 Acc:97.38%
Training: Epoch[009/035] Iteration[090/381] Loss: 0.0627 Acc:97.50%
Training: Epoch[009/035] Iteration[100/381] Loss: 0.0383 Acc:97.66%
Training: Epoch[009/035] Iteration[110/381] Loss: 0.0575 Acc:97.64%
Training: Epoch[009/035] Iteration[120/381] Loss: 0.0492 Acc:97.73%
Training: Epoch[009/035] Iteration[130/381] Loss: 0.0730 Acc:97.76%
Training: Epoch[009/035] Iteration[140/381] Loss: 0.0751 Acc:97.77%
Training: Epoch[009/035] Iteration[150/381] Loss: 0.0823 Acc:97.73%
Training: Epoch[009/035] Iteration[160/381] Loss: 0.0815 Acc:97.71%
Training: Epoch[009/035] Iteration[170/381] Loss: 0.0678 Acc:97.68%
Training: Epoch[009/035] Iteration[180/381] Loss: 0.0874 Acc:97.73%
Training: Epoch[009/035] Iteration[190/381] Loss: 0.0718 Acc:97.73%
Training: Epoch[009/035] Iteration[200/381] Loss: 0.0375 Acc:97.80%
Training: Epoch[009/035] Iteration[210/381] Loss: 0.0809 Acc:97.77%
Training: Epoch[009/035] Iteration[220/381] Loss: 0.0519 Acc:97.81%
Training: Epoch[009/035] Iteration[230/381] Loss: 0.0474 Acc:97.83%
Training: Epoch[009/035] Iteration[240/381] Loss: 0.0522 Acc:97.89%
Training: Epoch[009/035] Iteration[250/381] Loss: 0.0463 Acc:97.94%
Training: Epoch[009/035] Iteration[260/381] Loss: 0.1030 Acc:97.93%
Training: Epoch[009/035] Iteration[270/381] Loss: 0.1234 Acc:97.86%
Training: Epoch[009/035] Iteration[280/381] Loss: 0.0748 Acc:97.87%
Training: Epoch[009/035] Iteration[290/381] Loss: 0.1110 Acc:97.80%
Training: Epoch[009/035] Iteration[300/381] Loss: 0.1050 Acc:97.78%
Training: Epoch[009/035] Iteration[310/381] Loss: 0.0776 Acc:97.77%
Training: Epoch[009/035] Iteration[320/381] Loss: 0.0466 Acc:97.80%
Training: Epoch[009/035] Iteration[330/381] Loss: 0.0919 Acc:97.77%
Training: Epoch[009/035] Iteration[340/381] Loss: 0.0977 Acc:97.78%
Training: Epoch[009/035] Iteration[350/381] Loss: 0.1701 Acc:97.76%
Training: Epoch[009/035] Iteration[360/381] Loss: 0.1105 Acc:97.75%
Training: Epoch[009/035] Iteration[370/381] Loss: 0.0709 Acc:97.73%
Training: Epoch[009/035] Iteration[380/381] Loss: 0.0638 Acc:97.75%
Valid set Accuracy:85.09%
Training: Epoch[010/035] Iteration[010/381] Loss: 0.0614 Acc:98.12%
Training: Epoch[010/035] Iteration[020/381] Loss: 0.0319 Acc:98.75%
Training: Epoch[010/035] Iteration[030/381] Loss: 0.0654 Acc:98.44%
Training: Epoch[010/035] Iteration[040/381] Loss: 0.0508 Acc:98.44%
Training: Epoch[010/035] Iteration[050/381] Loss: 0.0423 Acc:98.50%
Training: Epoch[010/035] Iteration[060/381] Loss: 0.0195 Acc:98.65%
Training: Epoch[010/035] Iteration[070/381] Loss: 0.0483 Acc:98.62%
Training: Epoch[010/035] Iteration[080/381] Loss: 0.0555 Acc:98.52%
Training: Epoch[010/035] Iteration[090/381] Loss: 0.0692 Acc:98.40%
Training: Epoch[010/035] Iteration[100/381] Loss: 0.0390 Acc:98.44%
Training: Epoch[010/035] Iteration[110/381] Loss: 0.0265 Acc:98.52%
Training: Epoch[010/035] Iteration[120/381] Loss: 0.0393 Acc:98.54%
Training: Epoch[010/035] Iteration[130/381] Loss: 0.0891 Acc:98.51%
Training: Epoch[010/035] Iteration[140/381] Loss: 0.0828 Acc:98.42%
Training: Epoch[010/035] Iteration[150/381] Loss: 0.1187 Acc:98.29%
Training: Epoch[010/035] Iteration[160/381] Loss: 0.0915 Acc:98.24%
Training: Epoch[010/035] Iteration[170/381] Loss: 0.1058 Acc:98.14%
Training: Epoch[010/035] Iteration[180/381] Loss: 0.0951 Acc:98.09%
Training: Epoch[010/035] Iteration[190/381] Loss: 0.1038 Acc:98.01%
Training: Epoch[010/035] Iteration[200/381] Loss: 0.0653 Acc:98.05%
Training: Epoch[010/035] Iteration[210/381] Loss: 0.1409 Acc:97.96%
Training: Epoch[010/035] Iteration[220/381] Loss: 0.0637 Acc:97.98%
Training: Epoch[010/035] Iteration[230/381] Loss: 0.0813 Acc:97.96%
Training: Epoch[010/035] Iteration[240/381] Loss: 0.0329 Acc:98.02%
Training: Epoch[010/035] Iteration[250/381] Loss: 0.1038 Acc:98.02%
Training: Epoch[010/035] Iteration[260/381] Loss: 0.0505 Acc:98.03%
Training: Epoch[010/035] Iteration[270/381] Loss: 0.0229 Acc:98.08%
Training: Epoch[010/035] Iteration[280/381] Loss: 0.0346 Acc:98.10%
Training: Epoch[010/035] Iteration[290/381] Loss: 0.0259 Acc:98.15%
Training: Epoch[010/035] Iteration[300/381] Loss: 0.0480 Acc:98.16%
Training: Epoch[010/035] Iteration[310/381] Loss: 0.0198 Acc:98.21%
Training: Epoch[010/035] Iteration[320/381] Loss: 0.1297 Acc:98.15%
Training: Epoch[010/035] Iteration[330/381] Loss: 0.1110 Acc:98.12%
Training: Epoch[010/035] Iteration[340/381] Loss: 0.1223 Acc:98.08%
Training: Epoch[010/035] Iteration[350/381] Loss: 0.1176 Acc:98.04%
Training: Epoch[010/035] Iteration[360/381] Loss: 0.1288 Acc:97.99%
Training: Epoch[010/035] Iteration[370/381] Loss: 0.1006 Acc:97.96%
Training: Epoch[010/035] Iteration[380/381] Loss: 0.0738 Acc:97.97%
Training: Epoch[011/035] Iteration[010/381] Loss: 0.0281 Acc:99.69%
Training: Epoch[011/035] Iteration[020/381] Loss: 0.0556 Acc:99.22%
Training: Epoch[011/035] Iteration[030/381] Loss: 0.0649 Acc:99.06%
Training: Epoch[011/035] Iteration[040/381] Loss: 0.0313 Acc:98.98%
Training: Epoch[011/035] Iteration[050/381] Loss: 0.0439 Acc:99.00%
Training: Epoch[011/035] Iteration[060/381] Loss: 0.0386 Acc:98.96%
Training: Epoch[011/035] Iteration[070/381] Loss: 0.0629 Acc:98.88%
Training: Epoch[011/035] Iteration[080/381] Loss: 0.0843 Acc:98.79%
Training: Epoch[011/035] Iteration[090/381] Loss: 0.0830 Acc:98.68%
Training: Epoch[011/035] Iteration[100/381] Loss: 0.0970 Acc:98.47%
Training: Epoch[011/035] Iteration[110/381] Loss: 0.0971 Acc:98.38%
Training: Epoch[011/035] Iteration[120/381] Loss: 0.0324 Acc:98.46%
Training: Epoch[011/035] Iteration[130/381] Loss: 0.0641 Acc:98.39%
Training: Epoch[011/035] Iteration[140/381] Loss: 0.0420 Acc:98.42%
Training: Epoch[011/035] Iteration[150/381] Loss: 0.0667 Acc:98.42%
Training: Epoch[011/035] Iteration[160/381] Loss: 0.0660 Acc:98.42%
Training: Epoch[011/035] Iteration[170/381] Loss: 0.1040 Acc:98.40%
Training: Epoch[011/035] Iteration[180/381] Loss: 0.0748 Acc:98.35%
Training: Epoch[011/035] Iteration[190/381] Loss: 0.0656 Acc:98.40%
Training: Epoch[011/035] Iteration[200/381] Loss: 0.0438 Acc:98.41%
Training: Epoch[011/035] Iteration[210/381] Loss: 0.0580 Acc:98.45%
Training: Epoch[011/035] Iteration[220/381] Loss: 0.0615 Acc:98.42%
Training: Epoch[011/035] Iteration[230/381] Loss: 0.0529 Acc:98.40%
Training: Epoch[011/035] Iteration[240/381] Loss: 0.0667 Acc:98.36%
Training: Epoch[011/035] Iteration[250/381] Loss: 0.0277 Acc:98.41%
Training: Epoch[011/035] Iteration[260/381] Loss: 0.0574 Acc:98.43%
Training: Epoch[011/035] Iteration[270/381] Loss: 0.0273 Acc:98.45%
Training: Epoch[011/035] Iteration[280/381] Loss: 0.0246 Acc:98.49%
Training: Epoch[011/035] Iteration[290/381] Loss: 0.0578 Acc:98.50%
Training: Epoch[011/035] Iteration[300/381] Loss: 0.0854 Acc:98.50%
Training: Epoch[011/035] Iteration[310/381] Loss: 0.0149 Acc:98.54%
Training: Epoch[011/035] Iteration[320/381] Loss: 0.0353 Acc:98.55%
Training: Epoch[011/035] Iteration[330/381] Loss: 0.0243 Acc:98.59%
Training: Epoch[011/035] Iteration[340/381] Loss: 0.0132 Acc:98.62%
Training: Epoch[011/035] Iteration[350/381] Loss: 0.0428 Acc:98.63%
Training: Epoch[011/035] Iteration[360/381] Loss: 0.0158 Acc:98.65%
Training: Epoch[011/035] Iteration[370/381] Loss: 0.0659 Acc:98.65%
Training: Epoch[011/035] Iteration[380/381] Loss: 0.0409 Acc:98.65%
Valid set Accuracy:88.15%
Training: Epoch[012/035] Iteration[010/381] Loss: 0.0412 Acc:99.38%
Training: Epoch[012/035] Iteration[020/381] Loss: 0.0348 Acc:99.22%
Training: Epoch[012/035] Iteration[030/381] Loss: 0.0087 Acc:99.38%
Training: Epoch[012/035] Iteration[040/381] Loss: 0.0348 Acc:99.38%
Training: Epoch[012/035] Iteration[050/381] Loss: 0.0209 Acc:99.38%
Training: Epoch[012/035] Iteration[060/381] Loss: 0.0142 Acc:99.43%
Training: Epoch[012/035] Iteration[070/381] Loss: 0.0193 Acc:99.46%
Training: Epoch[012/035] Iteration[080/381] Loss: 0.0405 Acc:99.49%
Training: Epoch[012/035] Iteration[090/381] Loss: 0.0176 Acc:99.51%
Training: Epoch[012/035] Iteration[100/381] Loss: 0.0061 Acc:99.56%
Training: Epoch[012/035] Iteration[110/381] Loss: 0.0132 Acc:99.55%
Training: Epoch[012/035] Iteration[120/381] Loss: 0.0327 Acc:99.48%
Training: Epoch[012/035] Iteration[130/381] Loss: 0.0290 Acc:99.42%
Training: Epoch[012/035] Iteration[140/381] Loss: 0.0624 Acc:99.40%
Training: Epoch[012/035] Iteration[150/381] Loss: 0.0308 Acc:99.35%
Training: Epoch[012/035] Iteration[160/381] Loss: 0.0276 Acc:99.36%
Training: Epoch[012/035] Iteration[170/381] Loss: 0.0148 Acc:99.38%
Training: Epoch[012/035] Iteration[180/381] Loss: 0.0135 Acc:99.38%
Training: Epoch[012/035] Iteration[190/381] Loss: 0.0195 Acc:99.38%
Training: Epoch[012/035] Iteration[200/381] Loss: 0.0215 Acc:99.39%
Training: Epoch[012/035] Iteration[210/381] Loss: 0.0099 Acc:99.40%
Training: Epoch[012/035] Iteration[220/381] Loss: 0.0581 Acc:99.36%
Training: Epoch[012/035] Iteration[230/381] Loss: 0.0243 Acc:99.38%
Training: Epoch[012/035] Iteration[240/381] Loss: 0.0064 Acc:99.40%
Training: Epoch[012/035] Iteration[250/381] Loss: 0.0417 Acc:99.39%
Training: Epoch[012/035] Iteration[260/381] Loss: 0.0184 Acc:99.41%
Training: Epoch[012/035] Iteration[270/381] Loss: 0.0409 Acc:99.39%
Training: Epoch[012/035] Iteration[280/381] Loss: 0.0120 Acc:99.40%
Training: Epoch[012/035] Iteration[290/381] Loss: 0.0157 Acc:99.40%
Training: Epoch[012/035] Iteration[300/381] Loss: 0.0126 Acc:99.41%
Training: Epoch[012/035] Iteration[310/381] Loss: 0.0255 Acc:99.42%
Training: Epoch[012/035] Iteration[320/381] Loss: 0.0601 Acc:99.38%
Training: Epoch[012/035] Iteration[330/381] Loss: 0.0635 Acc:99.33%
Training: Epoch[012/035] Iteration[340/381] Loss: 0.0456 Acc:99.31%
Training: Epoch[012/035] Iteration[350/381] Loss: 0.0602 Acc:99.29%
Training: Epoch[012/035] Iteration[360/381] Loss: 0.0863 Acc:99.25%
Training: Epoch[012/035] Iteration[370/381] Loss: 0.0294 Acc:99.23%
Training: Epoch[012/035] Iteration[380/381] Loss: 0.1325 Acc:99.19%
Training: Epoch[013/035] Iteration[010/381] Loss: 0.0427 Acc:98.44%
Training: Epoch[013/035] Iteration[020/381] Loss: 0.1390 Acc:97.97%
Training: Epoch[013/035] Iteration[030/381] Loss: 0.0299 Acc:98.44%
Training: Epoch[013/035] Iteration[040/381] Loss: 0.0476 Acc:98.52%
Training: Epoch[013/035] Iteration[050/381] Loss: 0.0198 Acc:98.75%
Training: Epoch[013/035] Iteration[060/381] Loss: 0.0281 Acc:98.91%
Training: Epoch[013/035] Iteration[070/381] Loss: 0.0141 Acc:99.02%
Training: Epoch[013/035] Iteration[080/381] Loss: 0.0311 Acc:99.06%
Training: Epoch[013/035] Iteration[090/381] Loss: 0.0419 Acc:99.06%
Training: Epoch[013/035] Iteration[100/381] Loss: 0.0160 Acc:99.12%
Training: Epoch[013/035] Iteration[110/381] Loss: 0.0189 Acc:99.18%
Training: Epoch[013/035] Iteration[120/381] Loss: 0.0096 Acc:99.22%
Training: Epoch[013/035] Iteration[130/381] Loss: 0.0533 Acc:99.18%
Training: Epoch[013/035] Iteration[140/381] Loss: 0.0280 Acc:99.17%
Training: Epoch[013/035] Iteration[150/381] Loss: 0.1014 Acc:99.12%
Training: Epoch[013/035] Iteration[160/381] Loss: 0.0245 Acc:99.14%
Training: Epoch[013/035] Iteration[170/381] Loss: 0.0261 Acc:99.12%
Training: Epoch[013/035] Iteration[180/381] Loss: 0.0267 Acc:99.13%
Training: Epoch[013/035] Iteration[190/381] Loss: 0.0133 Acc:99.18%
Training: Epoch[013/035] Iteration[200/381] Loss: 0.0098 Acc:99.20%
Training: Epoch[013/035] Iteration[210/381] Loss: 0.0114 Acc:99.23%
Training: Epoch[013/035] Iteration[220/381] Loss: 0.0118 Acc:99.23%
Training: Epoch[013/035] Iteration[230/381] Loss: 0.0206 Acc:99.23%
Training: Epoch[013/035] Iteration[240/381] Loss: 0.0294 Acc:99.22%
Training: Epoch[013/035] Iteration[250/381] Loss: 0.0391 Acc:99.21%
Training: Epoch[013/035] Iteration[260/381] Loss: 0.0192 Acc:99.22%
Training: Epoch[013/035] Iteration[270/381] Loss: 0.0401 Acc:99.21%
Training: Epoch[013/035] Iteration[280/381] Loss: 0.0449 Acc:99.20%
Training: Epoch[013/035] Iteration[290/381] Loss: 0.0329 Acc:99.18%
Training: Epoch[013/035] Iteration[300/381] Loss: 0.0199 Acc:99.18%
Training: Epoch[013/035] Iteration[310/381] Loss: 0.0214 Acc:99.18%
Training: Epoch[013/035] Iteration[320/381] Loss: 0.0484 Acc:99.18%
Training: Epoch[013/035] Iteration[330/381] Loss: 0.0760 Acc:99.14%
Training: Epoch[013/035] Iteration[340/381] Loss: 0.1179 Acc:99.12%
Training: Epoch[013/035] Iteration[350/381] Loss: 0.0983 Acc:99.06%
Training: Epoch[013/035] Iteration[360/381] Loss: 0.0762 Acc:99.05%
Training: Epoch[013/035] Iteration[370/381] Loss: 0.0370 Acc:99.05%
Training: Epoch[013/035] Iteration[380/381] Loss: 0.0176 Acc:99.06%
Valid set Accuracy:88.95%
Training: Epoch[014/035] Iteration[010/381] Loss: 0.0052 Acc:100.00%
Training: Epoch[014/035] Iteration[020/381] Loss: 0.0215 Acc:99.69%
Training: Epoch[014/035] Iteration[030/381] Loss: 0.0061 Acc:99.69%
Training: Epoch[014/035] Iteration[040/381] Loss: 0.0226 Acc:99.61%
Training: Epoch[014/035] Iteration[050/381] Loss: 0.0097 Acc:99.62%
Training: Epoch[014/035] Iteration[060/381] Loss: 0.0157 Acc:99.58%
Training: Epoch[014/035] Iteration[070/381] Loss: 0.0084 Acc:99.60%
Training: Epoch[014/035] Iteration[080/381] Loss: 0.0146 Acc:99.61%
Training: Epoch[014/035] Iteration[090/381] Loss: 0.0292 Acc:99.58%
Training: Epoch[014/035] Iteration[100/381] Loss: 0.0349 Acc:99.50%
Training: Epoch[014/035] Iteration[110/381] Loss: 0.0106 Acc:99.55%
Training: Epoch[014/035] Iteration[120/381] Loss: 0.0069 Acc:99.58%
Training: Epoch[014/035] Iteration[130/381] Loss: 0.0402 Acc:99.50%
Training: Epoch[014/035] Iteration[140/381] Loss: 0.0568 Acc:99.49%
Training: Epoch[014/035] Iteration[150/381] Loss: 0.0596 Acc:99.48%
Training: Epoch[014/035] Iteration[160/381] Loss: 0.0093 Acc:99.49%
Training: Epoch[014/035] Iteration[170/381] Loss: 0.0504 Acc:99.45%
Training: Epoch[014/035] Iteration[180/381] Loss: 0.0234 Acc:99.46%
Training: Epoch[014/035] Iteration[190/381] Loss: 0.0186 Acc:99.46%
Training: Epoch[014/035] Iteration[200/381] Loss: 0.0104 Acc:99.47%
Training: Epoch[014/035] Iteration[210/381] Loss: 0.0105 Acc:99.48%
Training: Epoch[014/035] Iteration[220/381] Loss: 0.0443 Acc:99.43%
Training: Epoch[014/035] Iteration[230/381] Loss: 0.0057 Acc:99.46%
Training: Epoch[014/035] Iteration[240/381] Loss: 0.0095 Acc:99.48%
Training: Epoch[014/035] Iteration[250/381] Loss: 0.0099 Acc:99.50%
Training: Epoch[014/035] Iteration[260/381] Loss: 0.0143 Acc:99.50%
Training: Epoch[014/035] Iteration[270/381] Loss: 0.0061 Acc:99.51%
Training: Epoch[014/035] Iteration[280/381] Loss: 0.0390 Acc:99.51%
Training: Epoch[014/035] Iteration[290/381] Loss: 0.0080 Acc:99.53%
Training: Epoch[014/035] Iteration[300/381] Loss: 0.0129 Acc:99.53%
Training: Epoch[014/035] Iteration[310/381] Loss: 0.0090 Acc:99.54%
Training: Epoch[014/035] Iteration[320/381] Loss: 0.0101 Acc:99.54%
Training: Epoch[014/035] Iteration[330/381] Loss: 0.0692 Acc:99.52%
Training: Epoch[014/035] Iteration[340/381] Loss: 0.0335 Acc:99.49%
Training: Epoch[014/035] Iteration[350/381] Loss: 0.0090 Acc:99.49%
Training: Epoch[014/035] Iteration[360/381] Loss: 0.0170 Acc:99.48%
Training: Epoch[014/035] Iteration[370/381] Loss: 0.0354 Acc:99.46%
Training: Epoch[014/035] Iteration[380/381] Loss: 0.0305 Acc:99.45%
Training: Epoch[015/035] Iteration[010/381] Loss: 0.0161 Acc:99.38%
Training: Epoch[015/035] Iteration[020/381] Loss: 0.0199 Acc:99.53%
Training: Epoch[015/035] Iteration[030/381] Loss: 0.0368 Acc:99.38%
Training: Epoch[015/035] Iteration[040/381] Loss: 0.0159 Acc:99.38%
Training: Epoch[015/035] Iteration[050/381] Loss: 0.0100 Acc:99.50%
Training: Epoch[015/035] Iteration[060/381] Loss: 0.0195 Acc:99.53%
Training: Epoch[015/035] Iteration[070/381] Loss: 0.0254 Acc:99.42%
Training: Epoch[015/035] Iteration[080/381] Loss: 0.0063 Acc:99.49%
Training: Epoch[015/035] Iteration[090/381] Loss: 0.0125 Acc:99.51%
Training: Epoch[015/035] Iteration[100/381] Loss: 0.0539 Acc:99.53%
Training: Epoch[015/035] Iteration[110/381] Loss: 0.0188 Acc:99.46%
Training: Epoch[015/035] Iteration[120/381] Loss: 0.0174 Acc:99.45%
Training: Epoch[015/035] Iteration[130/381] Loss: 0.0229 Acc:99.47%
Training: Epoch[015/035] Iteration[140/381] Loss: 0.0122 Acc:99.49%
Training: Epoch[015/035] Iteration[150/381] Loss: 0.0305 Acc:99.50%
Training: Epoch[015/035] Iteration[160/381] Loss: 0.0309 Acc:99.47%
Training: Epoch[015/035] Iteration[170/381] Loss: 0.0067 Acc:99.50%
Training: Epoch[015/035] Iteration[180/381] Loss: 0.0394 Acc:99.50%
Training: Epoch[015/035] Iteration[190/381] Loss: 0.0515 Acc:99.44%
Training: Epoch[015/035] Iteration[200/381] Loss: 0.0083 Acc:99.45%
Training: Epoch[015/035] Iteration[210/381] Loss: 0.0128 Acc:99.46%
Training: Epoch[015/035] Iteration[220/381] Loss: 0.0118 Acc:99.46%
Training: Epoch[015/035] Iteration[230/381] Loss: 0.0095 Acc:99.47%
Training: Epoch[015/035] Iteration[240/381] Loss: 0.0105 Acc:99.48%
Training: Epoch[015/035] Iteration[250/381] Loss: 0.0316 Acc:99.48%
Training: Epoch[015/035] Iteration[260/381] Loss: 0.0209 Acc:99.47%
Training: Epoch[015/035] Iteration[270/381] Loss: 0.0242 Acc:99.48%
Training: Epoch[015/035] Iteration[280/381] Loss: 0.0290 Acc:99.46%
Training: Epoch[015/035] Iteration[290/381] Loss: 0.0218 Acc:99.47%
Training: Epoch[015/035] Iteration[300/381] Loss: 0.0066 Acc:99.49%
Training: Epoch[015/035] Iteration[310/381] Loss: 0.0357 Acc:99.47%
Training: Epoch[015/035] Iteration[320/381] Loss: 0.0075 Acc:99.48%
Training: Epoch[015/035] Iteration[330/381] Loss: 0.0072 Acc:99.50%
Training: Epoch[015/035] Iteration[340/381] Loss: 0.0045 Acc:99.51%
Training: Epoch[015/035] Iteration[350/381] Loss: 0.0301 Acc:99.52%
Training: Epoch[015/035] Iteration[360/381] Loss: 0.0437 Acc:99.51%
Training: Epoch[015/035] Iteration[370/381] Loss: 0.0119 Acc:99.52%
Training: Epoch[015/035] Iteration[380/381] Loss: 0.0707 Acc:99.51%
Valid set Accuracy:88.81%
Training: Epoch[016/035] Iteration[010/381] Loss: 0.0077 Acc:100.00%
Training: Epoch[016/035] Iteration[020/381] Loss: 0.0048 Acc:100.00%
Training: Epoch[016/035] Iteration[030/381] Loss: 0.0042 Acc:100.00%
Training: Epoch[016/035] Iteration[040/381] Loss: 0.0064 Acc:100.00%
Training: Epoch[016/035] Iteration[050/381] Loss: 0.0041 Acc:100.00%
Training: Epoch[016/035] Iteration[060/381] Loss: 0.0009 Acc:100.00%
Training: Epoch[016/035] Iteration[070/381] Loss: 0.0027 Acc:100.00%
Training: Epoch[016/035] Iteration[080/381] Loss: 0.0021 Acc:100.00%
Training: Epoch[016/035] Iteration[090/381] Loss: 0.0027 Acc:100.00%
Training: Epoch[016/035] Iteration[100/381] Loss: 0.0017 Acc:100.00%
Training: Epoch[016/035] Iteration[110/381] Loss: 0.0052 Acc:99.97%
Training: Epoch[016/035] Iteration[120/381] Loss: 0.0010 Acc:99.97%
Training: Epoch[016/035] Iteration[130/381] Loss: 0.0207 Acc:99.93%
Training: Epoch[016/035] Iteration[140/381] Loss: 0.0073 Acc:99.91%
Training: Epoch[016/035] Iteration[150/381] Loss: 0.0068 Acc:99.90%
Training: Epoch[016/035] Iteration[160/381] Loss: 0.0029 Acc:99.90%
Training: Epoch[016/035] Iteration[170/381] Loss: 0.0175 Acc:99.87%
Training: Epoch[016/035] Iteration[180/381] Loss: 0.0014 Acc:99.88%
Training: Epoch[016/035] Iteration[190/381] Loss: 0.0052 Acc:99.88%
Training: Epoch[016/035] Iteration[200/381] Loss: 0.0071 Acc:99.88%
Training: Epoch[016/035] Iteration[210/381] Loss: 0.0035 Acc:99.88%
Training: Epoch[016/035] Iteration[220/381] Loss: 0.0012 Acc:99.89%
Training: Epoch[016/035] Iteration[230/381] Loss: 0.0130 Acc:99.88%
Training: Epoch[016/035] Iteration[240/381] Loss: 0.0168 Acc:99.87%
Training: Epoch[016/035] Iteration[250/381] Loss: 0.0157 Acc:99.86%
Training: Epoch[016/035] Iteration[260/381] Loss: 0.0017 Acc:99.87%
Training: Epoch[016/035] Iteration[270/381] Loss: 0.0416 Acc:99.86%
Training: Epoch[016/035] Iteration[280/381] Loss: 0.0113 Acc:99.85%
Training: Epoch[016/035] Iteration[290/381] Loss: 0.0015 Acc:99.86%
Training: Epoch[016/035] Iteration[300/381] Loss: 0.0075 Acc:99.85%
Training: Epoch[016/035] Iteration[310/381] Loss: 0.0742 Acc:99.83%
Training: Epoch[016/035] Iteration[320/381] Loss: 0.0032 Acc:99.83%
Training: Epoch[016/035] Iteration[330/381] Loss: 0.0168 Acc:99.81%
Training: Epoch[016/035] Iteration[340/381] Loss: 0.0081 Acc:99.81%
Training: Epoch[016/035] Iteration[350/381] Loss: 0.0111 Acc:99.80%
Training: Epoch[016/035] Iteration[360/381] Loss: 0.0082 Acc:99.80%
Training: Epoch[016/035] Iteration[370/381] Loss: 0.0159 Acc:99.80%
Training: Epoch[016/035] Iteration[380/381] Loss: 0.0020 Acc:99.80%
Training: Epoch[017/035] Iteration[010/381] Loss: 0.0103 Acc:99.69%
Training: Epoch[017/035] Iteration[020/381] Loss: 0.0017 Acc:99.84%
Training: Epoch[017/035] Iteration[030/381] Loss: 0.0072 Acc:99.79%
Training: Epoch[017/035] Iteration[040/381] Loss: 0.0024 Acc:99.84%
Training: Epoch[017/035] Iteration[050/381] Loss: 0.0252 Acc:99.81%
Training: Epoch[017/035] Iteration[060/381] Loss: 0.0013 Acc:99.84%
Training: Epoch[017/035] Iteration[070/381] Loss: 0.0058 Acc:99.87%
Training: Epoch[017/035] Iteration[080/381] Loss: 0.0022 Acc:99.88%
Training: Epoch[017/035] Iteration[090/381] Loss: 0.0010 Acc:99.90%
Training: Epoch[017/035] Iteration[100/381] Loss: 0.0069 Acc:99.91%
Training: Epoch[017/035] Iteration[110/381] Loss: 0.0005 Acc:99.91%
Training: Epoch[017/035] Iteration[120/381] Loss: 0.0003 Acc:99.92%
Training: Epoch[017/035] Iteration[130/381] Loss: 0.0014 Acc:99.93%
Training: Epoch[017/035] Iteration[140/381] Loss: 0.0006 Acc:99.93%
Training: Epoch[017/035] Iteration[150/381] Loss: 0.0010 Acc:99.94%
Training: Epoch[017/035] Iteration[160/381] Loss: 0.0007 Acc:99.94%
Training: Epoch[017/035] Iteration[170/381] Loss: 0.0004 Acc:99.94%
Training: Epoch[017/035] Iteration[180/381] Loss: 0.0007 Acc:99.95%
Training: Epoch[017/035] Iteration[190/381] Loss: 0.0010 Acc:99.95%
Training: Epoch[017/035] Iteration[200/381] Loss: 0.0008 Acc:99.95%
Training: Epoch[017/035] Iteration[210/381] Loss: 0.0008 Acc:99.96%
Training: Epoch[017/035] Iteration[220/381] Loss: 0.0006 Acc:99.96%
Training: Epoch[017/035] Iteration[230/381] Loss: 0.0038 Acc:99.95%
Training: Epoch[017/035] Iteration[240/381] Loss: 0.0007 Acc:99.95%
Training: Epoch[017/035] Iteration[250/381] Loss: 0.0148 Acc:99.92%
Training: Epoch[017/035] Iteration[260/381] Loss: 0.0041 Acc:99.93%
Training: Epoch[017/035] Iteration[270/381] Loss: 0.0136 Acc:99.92%
Training: Epoch[017/035] Iteration[280/381] Loss: 0.0074 Acc:99.92%
Training: Epoch[017/035] Iteration[290/381] Loss: 0.0083 Acc:99.91%
Training: Epoch[017/035] Iteration[300/381] Loss: 0.0129 Acc:99.90%
Training: Epoch[017/035] Iteration[310/381] Loss: 0.0018 Acc:99.90%
Training: Epoch[017/035] Iteration[320/381] Loss: 0.0026 Acc:99.90%
Training: Epoch[017/035] Iteration[330/381] Loss: 0.0029 Acc:99.91%
Training: Epoch[017/035] Iteration[340/381] Loss: 0.0207 Acc:99.90%
Training: Epoch[017/035] Iteration[350/381] Loss: 0.0030 Acc:99.90%
Training: Epoch[017/035] Iteration[360/381] Loss: 0.0025 Acc:99.90%
Training: Epoch[017/035] Iteration[370/381] Loss: 0.0012 Acc:99.91%
Training: Epoch[017/035] Iteration[380/381] Loss: 0.0010 Acc:99.91%
Valid set Accuracy:91.34%
Training: Epoch[018/035] Iteration[010/381] Loss: 0.0106 Acc:99.69%
Training: Epoch[018/035] Iteration[020/381] Loss: 0.0058 Acc:99.69%
Training: Epoch[018/035] Iteration[030/381] Loss: 0.0278 Acc:99.58%
Training: Epoch[018/035] Iteration[040/381] Loss: 0.0018 Acc:99.69%
Training: Epoch[018/035] Iteration[050/381] Loss: 0.0033 Acc:99.75%
Training: Epoch[018/035] Iteration[060/381] Loss: 0.0086 Acc:99.74%
Training: Epoch[018/035] Iteration[070/381] Loss: 0.0488 Acc:99.73%
Training: Epoch[018/035] Iteration[080/381] Loss: 0.0294 Acc:99.69%
Training: Epoch[018/035] Iteration[090/381] Loss: 0.0123 Acc:99.69%
Training: Epoch[018/035] Iteration[100/381] Loss: 0.0021 Acc:99.72%
Training: Epoch[018/035] Iteration[110/381] Loss: 0.0125 Acc:99.72%
Training: Epoch[018/035] Iteration[120/381] Loss: 0.0078 Acc:99.71%
Training: Epoch[018/035] Iteration[130/381] Loss: 0.0059 Acc:99.74%
Training: Epoch[018/035] Iteration[140/381] Loss: 0.0141 Acc:99.73%
Training: Epoch[018/035] Iteration[150/381] Loss: 0.0045 Acc:99.75%
Training: Epoch[018/035] Iteration[160/381] Loss: 0.0225 Acc:99.73%
Training: Epoch[018/035] Iteration[170/381] Loss: 0.0149 Acc:99.72%
Training: Epoch[018/035] Iteration[180/381] Loss: 0.0017 Acc:99.74%
Training: Epoch[018/035] Iteration[190/381] Loss: 0.0087 Acc:99.74%
Training: Epoch[018/035] Iteration[200/381] Loss: 0.0016 Acc:99.75%
Training: Epoch[018/035] Iteration[210/381] Loss: 0.0019 Acc:99.76%
Training: Epoch[018/035] Iteration[220/381] Loss: 0.0012 Acc:99.77%
Training: Epoch[018/035] Iteration[230/381] Loss: 0.0015 Acc:99.78%
Training: Epoch[018/035] Iteration[240/381] Loss: 0.0011 Acc:99.79%
Training: Epoch[018/035] Iteration[250/381] Loss: 0.0007 Acc:99.80%
Training: Epoch[018/035] Iteration[260/381] Loss: 0.0012 Acc:99.81%
Training: Epoch[018/035] Iteration[270/381] Loss: 0.0036 Acc:99.80%
Training: Epoch[018/035] Iteration[280/381] Loss: 0.0181 Acc:99.80%
Training: Epoch[018/035] Iteration[290/381] Loss: 0.0039 Acc:99.81%
Training: Epoch[018/035] Iteration[300/381] Loss: 0.0139 Acc:99.79%
Training: Epoch[018/035] Iteration[310/381] Loss: 0.0020 Acc:99.80%
Training: Epoch[018/035] Iteration[320/381] Loss: 0.0281 Acc:99.79%
Training: Epoch[018/035] Iteration[330/381] Loss: 0.0039 Acc:99.79%
Training: Epoch[018/035] Iteration[340/381] Loss: 0.0247 Acc:99.78%
Training: Epoch[018/035] Iteration[350/381] Loss: 0.0084 Acc:99.78%
Training: Epoch[018/035] Iteration[360/381] Loss: 0.0409 Acc:99.77%
Training: Epoch[018/035] Iteration[370/381] Loss: 0.0095 Acc:99.77%
Training: Epoch[018/035] Iteration[380/381] Loss: 0.0105 Acc:99.77%
Training: Epoch[019/035] Iteration[010/381] Loss: 0.0032 Acc:100.00%
Training: Epoch[019/035] Iteration[020/381] Loss: 0.0033 Acc:100.00%
Training: Epoch[019/035] Iteration[030/381] Loss: 0.0083 Acc:99.90%
Training: Epoch[019/035] Iteration[040/381] Loss: 0.0019 Acc:99.92%
Training: Epoch[019/035] Iteration[050/381] Loss: 0.0007 Acc:99.94%
Training: Epoch[019/035] Iteration[060/381] Loss: 0.0023 Acc:99.95%
Training: Epoch[019/035] Iteration[070/381] Loss: 0.0012 Acc:99.96%
Training: Epoch[019/035] Iteration[080/381] Loss: 0.0020 Acc:99.96%
Training: Epoch[019/035] Iteration[090/381] Loss: 0.0030 Acc:99.97%
Training: Epoch[019/035] Iteration[100/381] Loss: 0.0005 Acc:99.97%
Training: Epoch[019/035] Iteration[110/381] Loss: 0.0023 Acc:99.97%
Training: Epoch[019/035] Iteration[120/381] Loss: 0.0013 Acc:99.97%
Training: Epoch[019/035] Iteration[130/381] Loss: 0.0005 Acc:99.98%
Training: Epoch[019/035] Iteration[140/381] Loss: 0.0023 Acc:99.98%
Training: Epoch[019/035] Iteration[150/381] Loss: 0.0006 Acc:99.98%
Training: Epoch[019/035] Iteration[160/381] Loss: 0.0073 Acc:99.94%
Training: Epoch[019/035] Iteration[170/381] Loss: 0.0007 Acc:99.94%
Training: Epoch[019/035] Iteration[180/381] Loss: 0.0057 Acc:99.93%
Training: Epoch[019/035] Iteration[190/381] Loss: 0.0073 Acc:99.92%
Training: Epoch[019/035] Iteration[200/381] Loss: 0.0016 Acc:99.92%
Training: Epoch[019/035] Iteration[210/381] Loss: 0.0059 Acc:99.91%
Training: Epoch[019/035] Iteration[220/381] Loss: 0.0043 Acc:99.91%
Training: Epoch[019/035] Iteration[230/381] Loss: 0.0016 Acc:99.92%
Training: Epoch[019/035] Iteration[240/381] Loss: 0.0120 Acc:99.91%
Training: Epoch[019/035] Iteration[250/381] Loss: 0.0206 Acc:99.88%
Training: Epoch[019/035] Iteration[260/381] Loss: 0.0064 Acc:99.87%
Training: Epoch[019/035] Iteration[270/381] Loss: 0.0049 Acc:99.86%
Training: Epoch[019/035] Iteration[280/381] Loss: 0.0175 Acc:99.84%
Training: Epoch[019/035] Iteration[290/381] Loss: 0.0086 Acc:99.84%
Training: Epoch[019/035] Iteration[300/381] Loss: 0.0016 Acc:99.84%
Training: Epoch[019/035] Iteration[310/381] Loss: 0.0205 Acc:99.82%
Training: Epoch[019/035] Iteration[320/381] Loss: 0.0060 Acc:99.81%
Training: Epoch[019/035] Iteration[330/381] Loss: 0.0169 Acc:99.80%
Training: Epoch[019/035] Iteration[340/381] Loss: 0.0105 Acc:99.80%
Training: Epoch[019/035] Iteration[350/381] Loss: 0.0168 Acc:99.79%
Training: Epoch[019/035] Iteration[360/381] Loss: 0.0032 Acc:99.79%
Training: Epoch[019/035] Iteration[370/381] Loss: 0.0206 Acc:99.78%
Training: Epoch[019/035] Iteration[380/381] Loss: 0.0179 Acc:99.77%
Valid set Accuracy:87.88%
Training: Epoch[020/035] Iteration[010/381] Loss: 0.0253 Acc:99.69%
Training: Epoch[020/035] Iteration[020/381] Loss: 0.0356 Acc:99.69%
Training: Epoch[020/035] Iteration[030/381] Loss: 0.0049 Acc:99.79%
Training: Epoch[020/035] Iteration[040/381] Loss: 0.0249 Acc:99.69%
Training: Epoch[020/035] Iteration[050/381] Loss: 0.0049 Acc:99.75%
Training: Epoch[020/035] Iteration[060/381] Loss: 0.0035 Acc:99.79%
Training: Epoch[020/035] Iteration[070/381] Loss: 0.0215 Acc:99.78%
Training: Epoch[020/035] Iteration[080/381] Loss: 0.0188 Acc:99.73%
Training: Epoch[020/035] Iteration[090/381] Loss: 0.0039 Acc:99.76%
Training: Epoch[020/035] Iteration[100/381] Loss: 0.0505 Acc:99.75%
Training: Epoch[020/035] Iteration[110/381] Loss: 0.0214 Acc:99.72%
Training: Epoch[020/035] Iteration[120/381] Loss: 0.1140 Acc:99.61%
Training: Epoch[020/035] Iteration[130/381] Loss: 0.0310 Acc:99.54%
Training: Epoch[020/035] Iteration[140/381] Loss: 0.0195 Acc:99.55%
Training: Epoch[020/035] Iteration[150/381] Loss: 0.0130 Acc:99.56%
Training: Epoch[020/035] Iteration[160/381] Loss: 0.0192 Acc:99.55%
Training: Epoch[020/035] Iteration[170/381] Loss: 0.0126 Acc:99.56%
Training: Epoch[020/035] Iteration[180/381] Loss: 0.0305 Acc:99.55%
Training: Epoch[020/035] Iteration[190/381] Loss: 0.0143 Acc:99.56%
Training: Epoch[020/035] Iteration[200/381] Loss: 0.0049 Acc:99.56%
Training: Epoch[020/035] Iteration[210/381] Loss: 0.0059 Acc:99.58%
Training: Epoch[020/035] Iteration[220/381] Loss: 0.0337 Acc:99.57%
Training: Epoch[020/035] Iteration[230/381] Loss: 0.0366 Acc:99.55%
Training: Epoch[020/035] Iteration[240/381] Loss: 0.0086 Acc:99.56%
Training: Epoch[020/035] Iteration[250/381] Loss: 0.0129 Acc:99.56%
Training: Epoch[020/035] Iteration[260/381] Loss: 0.0051 Acc:99.58%
Training: Epoch[020/035] Iteration[270/381] Loss: 0.0041 Acc:99.59%
Training: Epoch[020/035] Iteration[280/381] Loss: 0.0223 Acc:99.60%
Training: Epoch[020/035] Iteration[290/381] Loss: 0.0321 Acc:99.60%
Training: Epoch[020/035] Iteration[300/381] Loss: 0.0055 Acc:99.61%
Training: Epoch[020/035] Iteration[310/381] Loss: 0.0265 Acc:99.60%
Training: Epoch[020/035] Iteration[320/381] Loss: 0.0440 Acc:99.55%
Training: Epoch[020/035] Iteration[330/381] Loss: 0.0435 Acc:99.52%
Training: Epoch[020/035] Iteration[340/381] Loss: 0.0071 Acc:99.52%
Training: Epoch[020/035] Iteration[350/381] Loss: 0.0128 Acc:99.53%
Training: Epoch[020/035] Iteration[360/381] Loss: 0.0548 Acc:99.53%
Training: Epoch[020/035] Iteration[370/381] Loss: 0.0023 Acc:99.54%
Training: Epoch[020/035] Iteration[380/381] Loss: 0.0097 Acc:99.55%
Training: Epoch[021/035] Iteration[010/381] Loss: 0.0081 Acc:99.69%
Training: Epoch[021/035] Iteration[020/381] Loss: 0.0097 Acc:99.69%
Training: Epoch[021/035] Iteration[030/381] Loss: 0.0069 Acc:99.69%
Training: Epoch[021/035] Iteration[040/381] Loss: 0.0024 Acc:99.77%
Training: Epoch[021/035] Iteration[050/381] Loss: 0.0065 Acc:99.81%
Training: Epoch[021/035] Iteration[060/381] Loss: 0.0007 Acc:99.84%
Training: Epoch[021/035] Iteration[070/381] Loss: 0.0013 Acc:99.87%
Training: Epoch[021/035] Iteration[080/381] Loss: 0.0049 Acc:99.84%
Training: Epoch[021/035] Iteration[090/381] Loss: 0.0029 Acc:99.86%
Training: Epoch[021/035] Iteration[100/381] Loss: 0.0017 Acc:99.88%
Training: Epoch[021/035] Iteration[110/381] Loss: 0.0025 Acc:99.89%
Training: Epoch[021/035] Iteration[120/381] Loss: 0.0007 Acc:99.90%
Training: Epoch[021/035] Iteration[130/381] Loss: 0.0006 Acc:99.90%
Training: Epoch[021/035] Iteration[140/381] Loss: 0.0015 Acc:99.91%
Training: Epoch[021/035] Iteration[150/381] Loss: 0.0006 Acc:99.92%
Training: Epoch[021/035] Iteration[160/381] Loss: 0.0017 Acc:99.92%
Training: Epoch[021/035] Iteration[170/381] Loss: 0.0006 Acc:99.93%
Training: Epoch[021/035] Iteration[180/381] Loss: 0.0027 Acc:99.93%
Training: Epoch[021/035] Iteration[190/381] Loss: 0.0054 Acc:99.92%
Training: Epoch[021/035] Iteration[200/381] Loss: 0.0242 Acc:99.91%
Training: Epoch[021/035] Iteration[210/381] Loss: 0.0015 Acc:99.91%
Training: Epoch[021/035] Iteration[220/381] Loss: 0.0018 Acc:99.91%
Training: Epoch[021/035] Iteration[230/381] Loss: 0.0014 Acc:99.92%
Training: Epoch[021/035] Iteration[240/381] Loss: 0.0102 Acc:99.91%
Training: Epoch[021/035] Iteration[250/381] Loss: 0.0080 Acc:99.90%
Training: Epoch[021/035] Iteration[260/381] Loss: 0.0071 Acc:99.89%
Training: Epoch[021/035] Iteration[270/381] Loss: 0.0258 Acc:99.86%
Training: Epoch[021/035] Iteration[280/381] Loss: 0.0192 Acc:99.85%
Training: Epoch[021/035] Iteration[290/381] Loss: 0.0046 Acc:99.86%
Training: Epoch[021/035] Iteration[300/381] Loss: 0.0212 Acc:99.82%
Training: Epoch[021/035] Iteration[310/381] Loss: 0.0162 Acc:99.81%
Training: Epoch[021/035] Iteration[320/381] Loss: 0.0307 Acc:99.78%
Training: Epoch[021/035] Iteration[330/381] Loss: 0.0211 Acc:99.76%
Training: Epoch[021/035] Iteration[340/381] Loss: 0.0228 Acc:99.75%
Training: Epoch[021/035] Iteration[350/381] Loss: 0.0115 Acc:99.75%
Training: Epoch[021/035] Iteration[360/381] Loss: 0.0056 Acc:99.76%
Training: Epoch[021/035] Iteration[370/381] Loss: 0.0017 Acc:99.76%
Training: Epoch[021/035] Iteration[380/381] Loss: 0.0028 Acc:99.77%
Valid set Accuracy:90.01%
Training: Epoch[022/035] Iteration[010/381] Loss: 0.0120 Acc:99.38%
Training: Epoch[022/035] Iteration[020/381] Loss: 0.0008 Acc:99.69%
Training: Epoch[022/035] Iteration[030/381] Loss: 0.0113 Acc:99.69%
Training: Epoch[022/035] Iteration[040/381] Loss: 0.0072 Acc:99.61%
Training: Epoch[022/035] Iteration[050/381] Loss: 0.0096 Acc:99.62%
Training: Epoch[022/035] Iteration[060/381] Loss: 0.0036 Acc:99.69%
Training: Epoch[022/035] Iteration[070/381] Loss: 0.0069 Acc:99.69%
Training: Epoch[022/035] Iteration[080/381] Loss: 0.0061 Acc:99.69%
Training: Epoch[022/035] Iteration[090/381] Loss: 0.0411 Acc:99.65%
Training: Epoch[022/035] Iteration[100/381] Loss: 0.0072 Acc:99.66%
Training: Epoch[022/035] Iteration[110/381] Loss: 0.0088 Acc:99.66%
Training: Epoch[022/035] Iteration[120/381] Loss: 0.0056 Acc:99.66%
Training: Epoch[022/035] Iteration[130/381] Loss: 0.0077 Acc:99.69%
Training: Epoch[022/035] Iteration[140/381] Loss: 0.0025 Acc:99.71%
Training: Epoch[022/035] Iteration[150/381] Loss: 0.0024 Acc:99.73%
Training: Epoch[022/035] Iteration[160/381] Loss: 0.0096 Acc:99.73%
Training: Epoch[022/035] Iteration[170/381] Loss: 0.0071 Acc:99.72%
Training: Epoch[022/035] Iteration[180/381] Loss: 0.0048 Acc:99.74%
Training: Epoch[022/035] Iteration[190/381] Loss: 0.0125 Acc:99.74%
Training: Epoch[022/035] Iteration[200/381] Loss: 0.0041 Acc:99.75%
Training: Epoch[022/035] Iteration[210/381] Loss: 0.0113 Acc:99.73%
Training: Epoch[022/035] Iteration[220/381] Loss: 0.0045 Acc:99.74%
Training: Epoch[022/035] Iteration[230/381] Loss: 0.0053 Acc:99.76%
Training: Epoch[022/035] Iteration[240/381] Loss: 0.0097 Acc:99.75%
Training: Epoch[022/035] Iteration[250/381] Loss: 0.0142 Acc:99.74%
Training: Epoch[022/035] Iteration[260/381] Loss: 0.0273 Acc:99.74%
Training: Epoch[022/035] Iteration[270/381] Loss: 0.0389 Acc:99.69%
Training: Epoch[022/035] Iteration[280/381] Loss: 0.0408 Acc:99.67%
Training: Epoch[022/035] Iteration[290/381] Loss: 0.0401 Acc:99.64%
Training: Epoch[022/035] Iteration[300/381] Loss: 0.0167 Acc:99.64%
Training: Epoch[022/035] Iteration[310/381] Loss: 0.0207 Acc:99.63%
Training: Epoch[022/035] Iteration[320/381] Loss: 0.0143 Acc:99.62%
Training: Epoch[022/035] Iteration[330/381] Loss: 0.0333 Acc:99.61%
Training: Epoch[022/035] Iteration[340/381] Loss: 0.0245 Acc:99.60%
Training: Epoch[022/035] Iteration[350/381] Loss: 0.0133 Acc:99.59%
Training: Epoch[022/035] Iteration[360/381] Loss: 0.0061 Acc:99.59%
Training: Epoch[022/035] Iteration[370/381] Loss: 0.0058 Acc:99.60%
Training: Epoch[022/035] Iteration[380/381] Loss: 0.0157 Acc:99.60%
Training: Epoch[023/035] Iteration[010/381] Loss: 0.0167 Acc:99.38%
Training: Epoch[023/035] Iteration[020/381] Loss: 0.0048 Acc:99.69%
Training: Epoch[023/035] Iteration[030/381] Loss: 0.0053 Acc:99.79%
Training: Epoch[023/035] Iteration[040/381] Loss: 0.0021 Acc:99.84%
Training: Epoch[023/035] Iteration[050/381] Loss: 0.0012 Acc:99.88%
Training: Epoch[023/035] Iteration[060/381] Loss: 0.0024 Acc:99.90%
Training: Epoch[023/035] Iteration[070/381] Loss: 0.0011 Acc:99.91%
Training: Epoch[023/035] Iteration[080/381] Loss: 0.0010 Acc:99.92%
Training: Epoch[023/035] Iteration[090/381] Loss: 0.0057 Acc:99.90%
Training: Epoch[023/035] Iteration[100/381] Loss: 0.0079 Acc:99.88%
Training: Epoch[023/035] Iteration[110/381] Loss: 0.0077 Acc:99.86%
Training: Epoch[023/035] Iteration[120/381] Loss: 0.0039 Acc:99.87%
Training: Epoch[023/035] Iteration[130/381] Loss: 0.0044 Acc:99.86%
Training: Epoch[023/035] Iteration[140/381] Loss: 0.0014 Acc:99.87%
Training: Epoch[023/035] Iteration[150/381] Loss: 0.0040 Acc:99.85%
Training: Epoch[023/035] Iteration[160/381] Loss: 0.0031 Acc:99.86%
Training: Epoch[023/035] Iteration[170/381] Loss: 0.0085 Acc:99.85%
Training: Epoch[023/035] Iteration[180/381] Loss: 0.0047 Acc:99.84%
Training: Epoch[023/035] Iteration[190/381] Loss: 0.0061 Acc:99.84%
Training: Epoch[023/035] Iteration[200/381] Loss: 0.0047 Acc:99.84%
Training: Epoch[023/035] Iteration[210/381] Loss: 0.0107 Acc:99.84%
Training: Epoch[023/035] Iteration[220/381] Loss: 0.0057 Acc:99.83%
Training: Epoch[023/035] Iteration[230/381] Loss: 0.0166 Acc:99.81%
Training: Epoch[023/035] Iteration[240/381] Loss: 0.0070 Acc:99.80%
Training: Epoch[023/035] Iteration[250/381] Loss: 0.0959 Acc:99.78%
Training: Epoch[023/035] Iteration[260/381] Loss: 0.0521 Acc:99.72%
Training: Epoch[023/035] Iteration[270/381] Loss: 0.0669 Acc:99.65%
Training: Epoch[023/035] Iteration[280/381] Loss: 0.0235 Acc:99.65%
Training: Epoch[023/035] Iteration[290/381] Loss: 0.0528 Acc:99.61%
Training: Epoch[023/035] Iteration[300/381] Loss: 0.0068 Acc:99.61%
Training: Epoch[023/035] Iteration[310/381] Loss: 0.0187 Acc:99.61%
Training: Epoch[023/035] Iteration[320/381] Loss: 0.0147 Acc:99.59%
Training: Epoch[023/035] Iteration[330/381] Loss: 0.0136 Acc:99.59%
Training: Epoch[023/035] Iteration[340/381] Loss: 0.0139 Acc:99.60%
Training: Epoch[023/035] Iteration[350/381] Loss: 0.0063 Acc:99.61%
Training: Epoch[023/035] Iteration[360/381] Loss: 0.0090 Acc:99.61%
Training: Epoch[023/035] Iteration[370/381] Loss: 0.0114 Acc:99.61%
Training: Epoch[023/035] Iteration[380/381] Loss: 0.0068 Acc:99.62%
Valid set Accuracy:89.21%
Training: Epoch[024/035] Iteration[010/381] Loss: 0.0349 Acc:99.38%
Training: Epoch[024/035] Iteration[020/381] Loss: 0.0176 Acc:99.38%
Training: Epoch[024/035] Iteration[030/381] Loss: 0.0055 Acc:99.58%
Training: Epoch[024/035] Iteration[040/381] Loss: 0.0180 Acc:99.45%
Training: Epoch[024/035] Iteration[050/381] Loss: 0.0063 Acc:99.50%
Training: Epoch[024/035] Iteration[060/381] Loss: 0.0052 Acc:99.58%
Training: Epoch[024/035] Iteration[070/381] Loss: 0.0026 Acc:99.64%
Training: Epoch[024/035] Iteration[080/381] Loss: 0.0014 Acc:99.69%
Training: Epoch[024/035] Iteration[090/381] Loss: 0.0032 Acc:99.72%
Training: Epoch[024/035] Iteration[100/381] Loss: 0.0030 Acc:99.75%
Training: Epoch[024/035] Iteration[110/381] Loss: 0.0104 Acc:99.74%
Training: Epoch[024/035] Iteration[120/381] Loss: 0.0023 Acc:99.77%
Training: Epoch[024/035] Iteration[130/381] Loss: 0.0016 Acc:99.78%
Training: Epoch[024/035] Iteration[140/381] Loss: 0.0044 Acc:99.80%
Training: Epoch[024/035] Iteration[150/381] Loss: 0.0077 Acc:99.79%
Training: Epoch[024/035] Iteration[160/381] Loss: 0.0007 Acc:99.80%
Training: Epoch[024/035] Iteration[170/381] Loss: 0.0016 Acc:99.82%
Training: Epoch[024/035] Iteration[180/381] Loss: 0.0015 Acc:99.83%
Training: Epoch[024/035] Iteration[190/381] Loss: 0.0026 Acc:99.84%
Training: Epoch[024/035] Iteration[200/381] Loss: 0.0012 Acc:99.84%
Training: Epoch[024/035] Iteration[210/381] Loss: 0.0025 Acc:99.85%
Training: Epoch[024/035] Iteration[220/381] Loss: 0.0033 Acc:99.84%
Training: Epoch[024/035] Iteration[230/381] Loss: 0.0008 Acc:99.85%
Training: Epoch[024/035] Iteration[240/381] Loss: 0.0072 Acc:99.84%
Training: Epoch[024/035] Iteration[250/381] Loss: 0.0024 Acc:99.85%
Training: Epoch[024/035] Iteration[260/381] Loss: 0.0324 Acc:99.84%
Training: Epoch[024/035] Iteration[270/381] Loss: 0.0033 Acc:99.85%
Training: Epoch[024/035] Iteration[280/381] Loss: 0.0136 Acc:99.84%
Training: Epoch[024/035] Iteration[290/381] Loss: 0.0010 Acc:99.85%
Training: Epoch[024/035] Iteration[300/381] Loss: 0.0063 Acc:99.84%
Training: Epoch[024/035] Iteration[310/381] Loss: 0.0036 Acc:99.85%
Training: Epoch[024/035] Iteration[320/381] Loss: 0.0003 Acc:99.85%
Training: Epoch[024/035] Iteration[330/381] Loss: 0.0007 Acc:99.86%
Training: Epoch[024/035] Iteration[340/381] Loss: 0.0006 Acc:99.86%
Training: Epoch[024/035] Iteration[350/381] Loss: 0.0022 Acc:99.87%
Training: Epoch[024/035] Iteration[360/381] Loss: 0.0017 Acc:99.87%
Training: Epoch[024/035] Iteration[370/381] Loss: 0.0011 Acc:99.87%
Training: Epoch[024/035] Iteration[380/381] Loss: 0.0017 Acc:99.88%
Training: Epoch[025/035] Iteration[010/381] Loss: 0.0002 Acc:100.00%
Training: Epoch[025/035] Iteration[020/381] Loss: 0.0002 Acc:100.00%
Training: Epoch[025/035] Iteration[030/381] Loss: 0.0004 Acc:100.00%
Training: Epoch[025/035] Iteration[040/381] Loss: 0.0009 Acc:100.00%
Training: Epoch[025/035] Iteration[050/381] Loss: 0.0012 Acc:100.00%
Training: Epoch[025/035] Iteration[060/381] Loss: 0.0002 Acc:100.00%
Training: Epoch[025/035] Iteration[070/381] Loss: 0.0007 Acc:100.00%
Training: Epoch[025/035] Iteration[080/381] Loss: 0.0002 Acc:100.00%
Training: Epoch[025/035] Iteration[090/381] Loss: 0.0002 Acc:100.00%
Training: Epoch[025/035] Iteration[100/381] Loss: 0.0002 Acc:100.00%
Training: Epoch[025/035] Iteration[110/381] Loss: 0.0007 Acc:100.00%
Training: Epoch[025/035] Iteration[120/381] Loss: 0.0010 Acc:100.00%
Training: Epoch[025/035] Iteration[130/381] Loss: 0.0002 Acc:100.00%
Training: Epoch[025/035] Iteration[140/381] Loss: 0.0003 Acc:100.00%
Training: Epoch[025/035] Iteration[150/381] Loss: 0.0002 Acc:100.00%
Training: Epoch[025/035] Iteration[160/381] Loss: 0.0003 Acc:100.00%
Training: Epoch[025/035] Iteration[170/381] Loss: 0.0002 Acc:100.00%
Training: Epoch[025/035] Iteration[180/381] Loss: 0.0004 Acc:100.00%
Training: Epoch[025/035] Iteration[190/381] Loss: 0.0002 Acc:100.00%
Training: Epoch[025/035] Iteration[200/381] Loss: 0.0001 Acc:100.00%
Training: Epoch[025/035] Iteration[210/381] Loss: 0.0002 Acc:100.00%
Training: Epoch[025/035] Iteration[220/381] Loss: 0.0002 Acc:100.00%
Training: Epoch[025/035] Iteration[230/381] Loss: 0.0004 Acc:100.00%
Training: Epoch[025/035] Iteration[240/381] Loss: 0.0003 Acc:100.00%
Training: Epoch[025/035] Iteration[250/381] Loss: 0.0001 Acc:100.00%
Training: Epoch[025/035] Iteration[260/381] Loss: 0.0001 Acc:100.00%
Training: Epoch[025/035] Iteration[270/381] Loss: 0.0002 Acc:100.00%
Training: Epoch[025/035] Iteration[280/381] Loss: 0.0001 Acc:100.00%
Training: Epoch[025/035] Iteration[290/381] Loss: 0.0062 Acc:99.99%
Training: Epoch[025/035] Iteration[300/381] Loss: 0.0192 Acc:99.98%
Training: Epoch[025/035] Iteration[310/381] Loss: 0.0004 Acc:99.98%
Training: Epoch[025/035] Iteration[320/381] Loss: 0.0006 Acc:99.98%
Training: Epoch[025/035] Iteration[330/381] Loss: 0.0008 Acc:99.98%
Training: Epoch[025/035] Iteration[340/381] Loss: 0.0023 Acc:99.98%
Training: Epoch[025/035] Iteration[350/381] Loss: 0.0003 Acc:99.98%
Training: Epoch[025/035] Iteration[360/381] Loss: 0.0140 Acc:99.97%
Training: Epoch[025/035] Iteration[370/381] Loss: 0.0207 Acc:99.97%
Training: Epoch[025/035] Iteration[380/381] Loss: 0.0180 Acc:99.95%
Valid set Accuracy:89.35%
Training: Epoch[026/035] Iteration[010/381] Loss: 0.0244 Acc:99.06%
Training: Epoch[026/035] Iteration[020/381] Loss: 0.0024 Acc:99.53%
Training: Epoch[026/035] Iteration[030/381] Loss: 0.0147 Acc:99.58%
Training: Epoch[026/035] Iteration[040/381] Loss: 0.0253 Acc:99.53%
Training: Epoch[026/035] Iteration[050/381] Loss: 0.0074 Acc:99.62%
Training: Epoch[026/035] Iteration[060/381] Loss: 0.0098 Acc:99.64%
Training: Epoch[026/035] Iteration[070/381] Loss: 0.0977 Acc:99.55%
Training: Epoch[026/035] Iteration[080/381] Loss: 0.0314 Acc:99.49%
Training: Epoch[026/035] Iteration[090/381] Loss: 0.0180 Acc:99.48%
Training: Epoch[026/035] Iteration[100/381] Loss: 0.0095 Acc:99.50%
Training: Epoch[026/035] Iteration[110/381] Loss: 0.0072 Acc:99.55%
Training: Epoch[026/035] Iteration[120/381] Loss: 0.0038 Acc:99.58%
Training: Epoch[026/035] Iteration[130/381] Loss: 0.0082 Acc:99.62%
Training: Epoch[026/035] Iteration[140/381] Loss: 0.0031 Acc:99.64%
Training: Epoch[026/035] Iteration[150/381] Loss: 0.0116 Acc:99.62%
Training: Epoch[026/035] Iteration[160/381] Loss: 0.0059 Acc:99.63%
Training: Epoch[026/035] Iteration[170/381] Loss: 0.0061 Acc:99.63%
Training: Epoch[026/035] Iteration[180/381] Loss: 0.0018 Acc:99.65%
Training: Epoch[026/035] Iteration[190/381] Loss: 0.0024 Acc:99.67%
Training: Epoch[026/035] Iteration[200/381] Loss: 0.0027 Acc:99.69%
Training: Epoch[026/035] Iteration[210/381] Loss: 0.0009 Acc:99.70%
Training: Epoch[026/035] Iteration[220/381] Loss: 0.0010 Acc:99.72%
Training: Epoch[026/035] Iteration[230/381] Loss: 0.0054 Acc:99.71%
Training: Epoch[026/035] Iteration[240/381] Loss: 0.0033 Acc:99.73%
Training: Epoch[026/035] Iteration[250/381] Loss: 0.0005 Acc:99.74%
Training: Epoch[026/035] Iteration[260/381] Loss: 0.0005 Acc:99.75%
Training: Epoch[026/035] Iteration[270/381] Loss: 0.0026 Acc:99.76%
Training: Epoch[026/035] Iteration[280/381] Loss: 0.0016 Acc:99.77%
Training: Epoch[026/035] Iteration[290/381] Loss: 0.0007 Acc:99.77%
Training: Epoch[026/035] Iteration[300/381] Loss: 0.0038 Acc:99.77%
Training: Epoch[026/035] Iteration[310/381] Loss: 0.0062 Acc:99.77%
Training: Epoch[026/035] Iteration[320/381] Loss: 0.0007 Acc:99.78%
Training: Epoch[026/035] Iteration[330/381] Loss: 0.0016 Acc:99.78%
Training: Epoch[026/035] Iteration[340/381] Loss: 0.0006 Acc:99.79%
Training: Epoch[026/035] Iteration[350/381] Loss: 0.0005 Acc:99.79%
Training: Epoch[026/035] Iteration[360/381] Loss: 0.0004 Acc:99.80%
Training: Epoch[026/035] Iteration[370/381] Loss: 0.0276 Acc:99.79%
Training: Epoch[026/035] Iteration[380/381] Loss: 0.0066 Acc:99.79%
Training: Epoch[027/035] Iteration[010/381] Loss: 0.0279 Acc:99.06%
Training: Epoch[027/035] Iteration[020/381] Loss: 0.0124 Acc:99.22%
Training: Epoch[027/035] Iteration[030/381] Loss: 0.0798 Acc:99.17%
Training: Epoch[027/035] Iteration[040/381] Loss: 0.0099 Acc:99.30%
Training: Epoch[027/035] Iteration[050/381] Loss: 0.0125 Acc:99.38%
Training: Epoch[027/035] Iteration[060/381] Loss: 0.0068 Acc:99.48%
Training: Epoch[027/035] Iteration[070/381] Loss: 0.0559 Acc:99.46%
Training: Epoch[027/035] Iteration[080/381] Loss: 0.0068 Acc:99.53%
Training: Epoch[027/035] Iteration[090/381] Loss: 0.0136 Acc:99.55%
Training: Epoch[027/035] Iteration[100/381] Loss: 0.0112 Acc:99.56%
Training: Epoch[027/035] Iteration[110/381] Loss: 0.0071 Acc:99.60%
Training: Epoch[027/035] Iteration[120/381] Loss: 0.0028 Acc:99.64%
Training: Epoch[027/035] Iteration[130/381] Loss: 0.0037 Acc:99.66%
Training: Epoch[027/035] Iteration[140/381] Loss: 0.0007 Acc:99.69%
Training: Epoch[027/035] Iteration[150/381] Loss: 0.0013 Acc:99.71%
Training: Epoch[027/035] Iteration[160/381] Loss: 0.0012 Acc:99.73%
Training: Epoch[027/035] Iteration[170/381] Loss: 0.0015 Acc:99.74%
Training: Epoch[027/035] Iteration[180/381] Loss: 0.0027 Acc:99.76%
Training: Epoch[027/035] Iteration[190/381] Loss: 0.0035 Acc:99.75%
Training: Epoch[027/035] Iteration[200/381] Loss: 0.0018 Acc:99.77%
Training: Epoch[027/035] Iteration[210/381] Loss: 0.0028 Acc:99.78%
Training: Epoch[027/035] Iteration[220/381] Loss: 0.0052 Acc:99.77%
Training: Epoch[027/035] Iteration[230/381] Loss: 0.0037 Acc:99.77%
Training: Epoch[027/035] Iteration[240/381] Loss: 0.0053 Acc:99.78%
Training: Epoch[027/035] Iteration[250/381] Loss: 0.0019 Acc:99.79%
Training: Epoch[027/035] Iteration[260/381] Loss: 0.0007 Acc:99.80%
Training: Epoch[027/035] Iteration[270/381] Loss: 0.0030 Acc:99.80%
Training: Epoch[027/035] Iteration[280/381] Loss: 0.0007 Acc:99.81%
Training: Epoch[027/035] Iteration[290/381] Loss: 0.0130 Acc:99.81%
Training: Epoch[027/035] Iteration[300/381] Loss: 0.0005 Acc:99.81%
Training: Epoch[027/035] Iteration[310/381] Loss: 0.0034 Acc:99.82%
Training: Epoch[027/035] Iteration[320/381] Loss: 0.0098 Acc:99.81%
Training: Epoch[027/035] Iteration[330/381] Loss: 0.0024 Acc:99.82%
Training: Epoch[027/035] Iteration[340/381] Loss: 0.0039 Acc:99.83%
Training: Epoch[027/035] Iteration[350/381] Loss: 0.0177 Acc:99.82%
Training: Epoch[027/035] Iteration[360/381] Loss: 0.0014 Acc:99.83%
Training: Epoch[027/035] Iteration[370/381] Loss: 0.0416 Acc:99.81%
Training: Epoch[027/035] Iteration[380/381] Loss: 0.0120 Acc:99.80%
Valid set Accuracy:87.48%
Training: Epoch[028/035] Iteration[010/381] Loss: 0.0098 Acc:99.69%
Training: Epoch[028/035] Iteration[020/381] Loss: 0.0256 Acc:99.53%
Training: Epoch[028/035] Iteration[030/381] Loss: 0.0087 Acc:99.58%
Training: Epoch[028/035] Iteration[040/381] Loss: 0.0364 Acc:99.45%
Training: Epoch[028/035] Iteration[050/381] Loss: 0.0039 Acc:99.56%
Training: Epoch[028/035] Iteration[060/381] Loss: 0.0437 Acc:99.48%
Training: Epoch[028/035] Iteration[070/381] Loss: 0.0018 Acc:99.55%
Training: Epoch[028/035] Iteration[080/381] Loss: 0.0122 Acc:99.57%
Training: Epoch[028/035] Iteration[090/381] Loss: 0.0061 Acc:99.58%
Training: Epoch[028/035] Iteration[100/381] Loss: 0.0057 Acc:99.59%
Training: Epoch[028/035] Iteration[110/381] Loss: 0.0109 Acc:99.60%
Training: Epoch[028/035] Iteration[120/381] Loss: 0.0056 Acc:99.61%
Training: Epoch[028/035] Iteration[130/381] Loss: 0.0159 Acc:99.62%
Training: Epoch[028/035] Iteration[140/381] Loss: 0.0061 Acc:99.62%
Training: Epoch[028/035] Iteration[150/381] Loss: 0.0381 Acc:99.60%
Training: Epoch[028/035] Iteration[160/381] Loss: 0.0061 Acc:99.63%
Training: Epoch[028/035] Iteration[170/381] Loss: 0.0157 Acc:99.63%
Training: Epoch[028/035] Iteration[180/381] Loss: 0.0019 Acc:99.65%
Training: Epoch[028/035] Iteration[190/381] Loss: 0.0015 Acc:99.67%
Training: Epoch[028/035] Iteration[200/381] Loss: 0.0079 Acc:99.69%
Training: Epoch[028/035] Iteration[210/381] Loss: 0.0041 Acc:99.69%
Training: Epoch[028/035] Iteration[220/381] Loss: 0.0130 Acc:99.69%
Training: Epoch[028/035] Iteration[230/381] Loss: 0.0074 Acc:99.69%
Training: Epoch[028/035] Iteration[240/381] Loss: 0.0024 Acc:99.70%
Training: Epoch[028/035] Iteration[250/381] Loss: 0.0058 Acc:99.71%
Training: Epoch[028/035] Iteration[260/381] Loss: 0.0104 Acc:99.71%
Training: Epoch[028/035] Iteration[270/381] Loss: 0.0220 Acc:99.70%
Training: Epoch[028/035] Iteration[280/381] Loss: 0.0032 Acc:99.71%
Training: Epoch[028/035] Iteration[290/381] Loss: 0.0088 Acc:99.71%
Training: Epoch[028/035] Iteration[300/381] Loss: 0.0132 Acc:99.71%
Training: Epoch[028/035] Iteration[310/381] Loss: 0.0086 Acc:99.71%
Training: Epoch[028/035] Iteration[320/381] Loss: 0.0043 Acc:99.71%
Training: Epoch[028/035] Iteration[330/381] Loss: 0.0024 Acc:99.72%
Training: Epoch[028/035] Iteration[340/381] Loss: 0.0060 Acc:99.72%
Training: Epoch[028/035] Iteration[350/381] Loss: 0.0181 Acc:99.71%
Training: Epoch[028/035] Iteration[360/381] Loss: 0.0186 Acc:99.70%
Training: Epoch[028/035] Iteration[370/381] Loss: 0.0151 Acc:99.70%
Training: Epoch[028/035] Iteration[380/381] Loss: 0.0039 Acc:99.70%
Training: Epoch[029/035] Iteration[010/381] Loss: 0.0017 Acc:100.00%
Training: Epoch[029/035] Iteration[020/381] Loss: 0.0041 Acc:100.00%
Training: Epoch[029/035] Iteration[030/381] Loss: 0.0068 Acc:99.90%
Training: Epoch[029/035] Iteration[040/381] Loss: 0.0073 Acc:99.84%
Training: Epoch[029/035] Iteration[050/381] Loss: 0.0024 Acc:99.88%
Training: Epoch[029/035] Iteration[060/381] Loss: 0.0446 Acc:99.74%
Training: Epoch[029/035] Iteration[070/381] Loss: 0.0029 Acc:99.78%
Training: Epoch[029/035] Iteration[080/381] Loss: 0.0531 Acc:99.69%
Training: Epoch[029/035] Iteration[090/381] Loss: 0.0027 Acc:99.72%
Training: Epoch[029/035] Iteration[100/381] Loss: 0.0022 Acc:99.75%
Training: Epoch[029/035] Iteration[110/381] Loss: 0.0081 Acc:99.77%
Training: Epoch[029/035] Iteration[120/381] Loss: 0.0023 Acc:99.79%
Training: Epoch[029/035] Iteration[130/381] Loss: 0.0035 Acc:99.81%
Training: Epoch[029/035] Iteration[140/381] Loss: 0.0047 Acc:99.80%
Training: Epoch[029/035] Iteration[150/381] Loss: 0.0030 Acc:99.81%
Training: Epoch[029/035] Iteration[160/381] Loss: 0.0009 Acc:99.82%
Training: Epoch[029/035] Iteration[170/381] Loss: 0.0034 Acc:99.83%
Training: Epoch[029/035] Iteration[180/381] Loss: 0.0006 Acc:99.84%
Training: Epoch[029/035] Iteration[190/381] Loss: 0.0048 Acc:99.84%
Training: Epoch[029/035] Iteration[200/381] Loss: 0.0037 Acc:99.83%
Training: Epoch[029/035] Iteration[210/381] Loss: 0.0112 Acc:99.82%
Training: Epoch[029/035] Iteration[220/381] Loss: 0.0024 Acc:99.83%
Training: Epoch[029/035] Iteration[230/381] Loss: 0.0204 Acc:99.82%
Training: Epoch[029/035] Iteration[240/381] Loss: 0.0035 Acc:99.83%
Training: Epoch[029/035] Iteration[250/381] Loss: 0.0135 Acc:99.83%
Training: Epoch[029/035] Iteration[260/381] Loss: 0.0241 Acc:99.82%
Training: Epoch[029/035] Iteration[270/381] Loss: 0.0010 Acc:99.83%
Training: Epoch[029/035] Iteration[280/381] Loss: 0.0015 Acc:99.83%
Training: Epoch[029/035] Iteration[290/381] Loss: 0.0147 Acc:99.82%
Training: Epoch[029/035] Iteration[300/381] Loss: 0.0089 Acc:99.81%
Training: Epoch[029/035] Iteration[310/381] Loss: 0.0252 Acc:99.81%
Training: Epoch[029/035] Iteration[320/381] Loss: 0.0041 Acc:99.81%
Training: Epoch[029/035] Iteration[330/381] Loss: 0.0022 Acc:99.82%
Training: Epoch[029/035] Iteration[340/381] Loss: 0.0014 Acc:99.83%
Training: Epoch[029/035] Iteration[350/381] Loss: 0.0012 Acc:99.83%
Training: Epoch[029/035] Iteration[360/381] Loss: 0.0017 Acc:99.84%
Training: Epoch[029/035] Iteration[370/381] Loss: 0.0019 Acc:99.84%
Training: Epoch[029/035] Iteration[380/381] Loss: 0.0005 Acc:99.84%
Valid set Accuracy:90.55%
Training: Epoch[030/035] Iteration[010/381] Loss: 0.0008 Acc:100.00%
Training: Epoch[030/035] Iteration[020/381] Loss: 0.0011 Acc:100.00%
Training: Epoch[030/035] Iteration[030/381] Loss: 0.0004 Acc:100.00%
Training: Epoch[030/035] Iteration[040/381] Loss: 0.0004 Acc:100.00%
Training: Epoch[030/035] Iteration[050/381] Loss: 0.0006 Acc:100.00%
Training: Epoch[030/035] Iteration[060/381] Loss: 0.0002 Acc:100.00%
Training: Epoch[030/035] Iteration[070/381] Loss: 0.0002 Acc:100.00%
Training: Epoch[030/035] Iteration[080/381] Loss: 0.0007 Acc:100.00%
Training: Epoch[030/035] Iteration[090/381] Loss: 0.0006 Acc:100.00%
Training: Epoch[030/035] Iteration[100/381] Loss: 0.0004 Acc:100.00%
Training: Epoch[030/035] Iteration[110/381] Loss: 0.0002 Acc:100.00%
Training: Epoch[030/035] Iteration[120/381] Loss: 0.0006 Acc:100.00%
Training: Epoch[030/035] Iteration[130/381] Loss: 0.0003 Acc:100.00%
Training: Epoch[030/035] Iteration[140/381] Loss: 0.0003 Acc:100.00%
Training: Epoch[030/035] Iteration[150/381] Loss: 0.0005 Acc:100.00%
Training: Epoch[030/035] Iteration[160/381] Loss: 0.0008 Acc:100.00%
Training: Epoch[030/035] Iteration[170/381] Loss: 0.0003 Acc:100.00%
Training: Epoch[030/035] Iteration[180/381] Loss: 0.0004 Acc:100.00%
Training: Epoch[030/035] Iteration[190/381] Loss: 0.0003 Acc:100.00%
Training: Epoch[030/035] Iteration[200/381] Loss: 0.0001 Acc:100.00%
Training: Epoch[030/035] Iteration[210/381] Loss: 0.0002 Acc:100.00%
Training: Epoch[030/035] Iteration[220/381] Loss: 0.0003 Acc:100.00%
Training: Epoch[030/035] Iteration[230/381] Loss: 0.0002 Acc:100.00%
Training: Epoch[030/035] Iteration[240/381] Loss: 0.0001 Acc:100.00%
Training: Epoch[030/035] Iteration[250/381] Loss: 0.0003 Acc:100.00%
Training: Epoch[030/035] Iteration[260/381] Loss: 0.0003 Acc:100.00%
Training: Epoch[030/035] Iteration[270/381] Loss: 0.0002 Acc:100.00%
Training: Epoch[030/035] Iteration[280/381] Loss: 0.0005 Acc:100.00%
Training: Epoch[030/035] Iteration[290/381] Loss: 0.0002 Acc:100.00%
Training: Epoch[030/035] Iteration[300/381] Loss: 0.0009 Acc:100.00%
Training: Epoch[030/035] Iteration[310/381] Loss: 0.0007 Acc:100.00%
Training: Epoch[030/035] Iteration[320/381] Loss: 0.0008 Acc:100.00%
Training: Epoch[030/035] Iteration[330/381] Loss: 0.0001 Acc:100.00%
Training: Epoch[030/035] Iteration[340/381] Loss: 0.0001 Acc:100.00%
Training: Epoch[030/035] Iteration[350/381] Loss: 0.0001 Acc:100.00%
Training: Epoch[030/035] Iteration[360/381] Loss: 0.0016 Acc:100.00%
Training: Epoch[030/035] Iteration[370/381] Loss: 0.0004 Acc:100.00%
Training: Epoch[030/035] Iteration[380/381] Loss: 0.0009 Acc:100.00%
Training: Epoch[031/035] Iteration[010/381] Loss: 0.0001 Acc:100.00%
Training: Epoch[031/035] Iteration[020/381] Loss: 0.0001 Acc:100.00%
Training: Epoch[031/035] Iteration[030/381] Loss: 0.0001 Acc:100.00%
Training: Epoch[031/035] Iteration[040/381] Loss: 0.0002 Acc:100.00%
Training: Epoch[031/035] Iteration[050/381] Loss: 0.0003 Acc:100.00%
Training: Epoch[031/035] Iteration[060/381] Loss: 0.0002 Acc:100.00%
Training: Epoch[031/035] Iteration[070/381] Loss: 0.0003 Acc:100.00%
Training: Epoch[031/035] Iteration[080/381] Loss: 0.0003 Acc:100.00%
Training: Epoch[031/035] Iteration[090/381] Loss: 0.0001 Acc:100.00%
Training: Epoch[031/035] Iteration[100/381] Loss: 0.0003 Acc:100.00%
Training: Epoch[031/035] Iteration[110/381] Loss: 0.0003 Acc:100.00%
Training: Epoch[031/035] Iteration[120/381] Loss: 0.0001 Acc:100.00%
Training: Epoch[031/035] Iteration[130/381] Loss: 0.0001 Acc:100.00%
Training: Epoch[031/035] Iteration[140/381] Loss: 0.0001 Acc:100.00%
Training: Epoch[031/035] Iteration[150/381] Loss: 0.0002 Acc:100.00%
Training: Epoch[031/035] Iteration[160/381] Loss: 0.0003 Acc:100.00%
Training: Epoch[031/035] Iteration[170/381] Loss: 0.0001 Acc:100.00%
Training: Epoch[031/035] Iteration[180/381] Loss: 0.0002 Acc:100.00%
Training: Epoch[031/035] Iteration[190/381] Loss: 0.0001 Acc:100.00%
Training: Epoch[031/035] Iteration[200/381] Loss: 0.0003 Acc:100.00%
Training: Epoch[031/035] Iteration[210/381] Loss: 0.0001 Acc:100.00%
Training: Epoch[031/035] Iteration[220/381] Loss: 0.0001 Acc:100.00%
Training: Epoch[031/035] Iteration[230/381] Loss: 0.0002 Acc:100.00%
Training: Epoch[031/035] Iteration[240/381] Loss: 0.0001 Acc:100.00%
Training: Epoch[031/035] Iteration[250/381] Loss: 0.0002 Acc:100.00%
Training: Epoch[031/035] Iteration[260/381] Loss: 0.0001 Acc:100.00%
Training: Epoch[031/035] Iteration[270/381] Loss: 0.0001 Acc:100.00%
Training: Epoch[031/035] Iteration[280/381] Loss: 0.0002 Acc:100.00%
Training: Epoch[031/035] Iteration[290/381] Loss: 0.0001 Acc:100.00%
Training: Epoch[031/035] Iteration[300/381] Loss: 0.0001 Acc:100.00%
Training: Epoch[031/035] Iteration[310/381] Loss: 0.0001 Acc:100.00%
Training: Epoch[031/035] Iteration[320/381] Loss: 0.0001 Acc:100.00%
Training: Epoch[031/035] Iteration[330/381] Loss: 0.0001 Acc:100.00%
Training: Epoch[031/035] Iteration[340/381] Loss: 0.0001 Acc:100.00%
Training: Epoch[031/035] Iteration[350/381] Loss: 0.0001 Acc:100.00%
Training: Epoch[031/035] Iteration[360/381] Loss: 0.0001 Acc:100.00%
Training: Epoch[031/035] Iteration[370/381] Loss: 0.0001 Acc:100.00%
Training: Epoch[031/035] Iteration[380/381] Loss: 0.0001 Acc:100.00%
Valid set Accuracy:90.28%
Training: Epoch[032/035] Iteration[010/381] Loss: 0.0001 Acc:100.00%
Training: Epoch[032/035] Iteration[020/381] Loss: 0.0002 Acc:100.00%
Training: Epoch[032/035] Iteration[030/381] Loss: 0.0001 Acc:100.00%
Training: Epoch[032/035] Iteration[040/381] Loss: 0.0000 Acc:100.00%
Training: Epoch[032/035] Iteration[050/381] Loss: 0.0001 Acc:100.00%
Training: Epoch[032/035] Iteration[060/381] Loss: 0.0001 Acc:100.00%
Training: Epoch[032/035] Iteration[070/381] Loss: 0.0001 Acc:100.00%
Training: Epoch[032/035] Iteration[080/381] Loss: 0.0001 Acc:100.00%
Training: Epoch[032/035] Iteration[090/381] Loss: 0.0008 Acc:100.00%
Training: Epoch[032/035] Iteration[100/381] Loss: 0.0001 Acc:100.00%
Training: Epoch[032/035] Iteration[110/381] Loss: 0.0001 Acc:100.00%
Training: Epoch[032/035] Iteration[120/381] Loss: 0.0004 Acc:100.00%
Training: Epoch[032/035] Iteration[130/381] Loss: 0.0007 Acc:100.00%
Training: Epoch[032/035] Iteration[140/381] Loss: 0.0001 Acc:100.00%
Training: Epoch[032/035] Iteration[150/381] Loss: 0.0002 Acc:100.00%
Training: Epoch[032/035] Iteration[160/381] Loss: 0.0003 Acc:100.00%
Training: Epoch[032/035] Iteration[170/381] Loss: 0.0001 Acc:100.00%
Training: Epoch[032/035] Iteration[180/381] Loss: 0.0001 Acc:100.00%
Training: Epoch[032/035] Iteration[190/381] Loss: 0.0002 Acc:100.00%
Training: Epoch[032/035] Iteration[200/381] Loss: 0.0001 Acc:100.00%
Training: Epoch[032/035] Iteration[210/381] Loss: 0.0001 Acc:100.00%
Training: Epoch[032/035] Iteration[220/381] Loss: 0.0003 Acc:100.00%
Training: Epoch[032/035] Iteration[230/381] Loss: 0.0001 Acc:100.00%
Training: Epoch[032/035] Iteration[240/381] Loss: 0.0001 Acc:100.00%
Training: Epoch[032/035] Iteration[250/381] Loss: 0.0001 Acc:100.00%
Training: Epoch[032/035] Iteration[260/381] Loss: 0.0002 Acc:100.00%
Training: Epoch[032/035] Iteration[270/381] Loss: 0.0000 Acc:100.00%
Training: Epoch[032/035] Iteration[280/381] Loss: 0.0001 Acc:100.00%
Training: Epoch[032/035] Iteration[290/381] Loss: 0.0001 Acc:100.00%
Training: Epoch[032/035] Iteration[300/381] Loss: 0.0003 Acc:100.00%
Training: Epoch[032/035] Iteration[310/381] Loss: 0.0001 Acc:100.00%
Training: Epoch[032/035] Iteration[320/381] Loss: 0.0001 Acc:100.00%
Training: Epoch[032/035] Iteration[330/381] Loss: 0.0000 Acc:100.00%
Training: Epoch[032/035] Iteration[340/381] Loss: 0.0001 Acc:100.00%
Training: Epoch[032/035] Iteration[350/381] Loss: 0.0000 Acc:100.00%
Training: Epoch[032/035] Iteration[360/381] Loss: 0.0001 Acc:100.00%
Training: Epoch[032/035] Iteration[370/381] Loss: 0.0003 Acc:100.00%
Training: Epoch[032/035] Iteration[380/381] Loss: 0.0001 Acc:100.00%
Training: Epoch[033/035] Iteration[010/381] Loss: 0.0000 Acc:100.00%
Training: Epoch[033/035] Iteration[020/381] Loss: 0.0004 Acc:100.00%
Training: Epoch[033/035] Iteration[030/381] Loss: 0.0001 Acc:100.00%
Training: Epoch[033/035] Iteration[040/381] Loss: 0.0000 Acc:100.00%
Training: Epoch[033/035] Iteration[050/381] Loss: 0.0000 Acc:100.00%
Training: Epoch[033/035] Iteration[060/381] Loss: 0.0001 Acc:100.00%
Training: Epoch[033/035] Iteration[070/381] Loss: 0.0001 Acc:100.00%
Training: Epoch[033/035] Iteration[080/381] Loss: 0.0004 Acc:100.00%
Training: Epoch[033/035] Iteration[090/381] Loss: 0.0000 Acc:100.00%
Training: Epoch[033/035] Iteration[100/381] Loss: 0.0001 Acc:100.00%
Training: Epoch[033/035] Iteration[110/381] Loss: 0.0000 Acc:100.00%
Training: Epoch[033/035] Iteration[120/381] Loss: 0.0001 Acc:100.00%
Training: Epoch[033/035] Iteration[130/381] Loss: 0.0000 Acc:100.00%
Training: Epoch[033/035] Iteration[140/381] Loss: 0.0007 Acc:100.00%
Training: Epoch[033/035] Iteration[150/381] Loss: 0.0001 Acc:100.00%
Training: Epoch[033/035] Iteration[160/381] Loss: 0.0000 Acc:100.00%
Training: Epoch[033/035] Iteration[170/381] Loss: 0.0001 Acc:100.00%
Training: Epoch[033/035] Iteration[180/381] Loss: 0.0000 Acc:100.00%
Training: Epoch[033/035] Iteration[190/381] Loss: 0.0001 Acc:100.00%
Training: Epoch[033/035] Iteration[200/381] Loss: 0.0002 Acc:100.00%
Training: Epoch[033/035] Iteration[210/381] Loss: 0.0001 Acc:100.00%
Training: Epoch[033/035] Iteration[220/381] Loss: 0.0001 Acc:100.00%
Training: Epoch[033/035] Iteration[230/381] Loss: 0.0000 Acc:100.00%
Training: Epoch[033/035] Iteration[240/381] Loss: 0.0000 Acc:100.00%
Training: Epoch[033/035] Iteration[250/381] Loss: 0.0001 Acc:100.00%
Training: Epoch[033/035] Iteration[260/381] Loss: 0.0000 Acc:100.00%
Training: Epoch[033/035] Iteration[270/381] Loss: 0.0000 Acc:100.00%
Training: Epoch[033/035] Iteration[280/381] Loss: 0.0005 Acc:100.00%
Training: Epoch[033/035] Iteration[290/381] Loss: 0.0000 Acc:100.00%
Training: Epoch[033/035] Iteration[300/381] Loss: 0.0001 Acc:100.00%
Training: Epoch[033/035] Iteration[310/381] Loss: 0.0000 Acc:100.00%
Training: Epoch[033/035] Iteration[320/381] Loss: 0.0000 Acc:100.00%
Training: Epoch[033/035] Iteration[330/381] Loss: 0.0000 Acc:100.00%
Training: Epoch[033/035] Iteration[340/381] Loss: 0.0003 Acc:100.00%
Training: Epoch[033/035] Iteration[350/381] Loss: 0.0000 Acc:100.00%
Training: Epoch[033/035] Iteration[360/381] Loss: 0.0000 Acc:100.00%
Training: Epoch[033/035] Iteration[370/381] Loss: 0.0000 Acc:100.00%
Training: Epoch[033/035] Iteration[380/381] Loss: 0.0000 Acc:100.00%
Valid set Accuracy:90.55%
Training: Epoch[034/035] Iteration[010/381] Loss: 0.0000 Acc:100.00%
Training: Epoch[034/035] Iteration[020/381] Loss: 0.0001 Acc:100.00%
Training: Epoch[034/035] Iteration[030/381] Loss: 0.0000 Acc:100.00%
Training: Epoch[034/035] Iteration[040/381] Loss: 0.0001 Acc:100.00%
Training: Epoch[034/035] Iteration[050/381] Loss: 0.0000 Acc:100.00%
Training: Epoch[034/035] Iteration[060/381] Loss: 0.0000 Acc:100.00%
Training: Epoch[034/035] Iteration[070/381] Loss: 0.0000 Acc:100.00%
Training: Epoch[034/035] Iteration[080/381] Loss: 0.0000 Acc:100.00%
Training: Epoch[034/035] Iteration[090/381] Loss: 0.0001 Acc:100.00%
Training: Epoch[034/035] Iteration[100/381] Loss: 0.0001 Acc:100.00%
Training: Epoch[034/035] Iteration[110/381] Loss: 0.0001 Acc:100.00%
Training: Epoch[034/035] Iteration[120/381] Loss: 0.0000 Acc:100.00%
Training: Epoch[034/035] Iteration[130/381] Loss: 0.0000 Acc:100.00%
Training: Epoch[034/035] Iteration[140/381] Loss: 0.0000 Acc:100.00%
Training: Epoch[034/035] Iteration[150/381] Loss: 0.0000 Acc:100.00%
Training: Epoch[034/035] Iteration[160/381] Loss: 0.0000 Acc:100.00%
Training: Epoch[034/035] Iteration[170/381] Loss: 0.0000 Acc:100.00%
Training: Epoch[034/035] Iteration[180/381] Loss: 0.0000 Acc:100.00%
Training: Epoch[034/035] Iteration[190/381] Loss: 0.0000 Acc:100.00%
Training: Epoch[034/035] Iteration[200/381] Loss: 0.0000 Acc:100.00%
Training: Epoch[034/035] Iteration[210/381] Loss: 0.0001 Acc:100.00%
Training: Epoch[034/035] Iteration[220/381] Loss: 0.0000 Acc:100.00%
Training: Epoch[034/035] Iteration[230/381] Loss: 0.0000 Acc:100.00%
Training: Epoch[034/035] Iteration[240/381] Loss: 0.0001 Acc:100.00%
Training: Epoch[034/035] Iteration[250/381] Loss: 0.0000 Acc:100.00%
Training: Epoch[034/035] Iteration[260/381] Loss: 0.0000 Acc:100.00%
Training: Epoch[034/035] Iteration[270/381] Loss: 0.0000 Acc:100.00%
Training: Epoch[034/035] Iteration[280/381] Loss: 0.0001 Acc:100.00%
Training: Epoch[034/035] Iteration[290/381] Loss: 0.0000 Acc:100.00%
Training: Epoch[034/035] Iteration[300/381] Loss: 0.0000 Acc:100.00%
Training: Epoch[034/035] Iteration[310/381] Loss: 0.0000 Acc:100.00%
Training: Epoch[034/035] Iteration[320/381] Loss: 0.0000 Acc:100.00%
Training: Epoch[034/035] Iteration[330/381] Loss: 0.0001 Acc:100.00%
Training: Epoch[034/035] Iteration[340/381] Loss: 0.0001 Acc:100.00%
Training: Epoch[034/035] Iteration[350/381] Loss: 0.0001 Acc:100.00%
Training: Epoch[034/035] Iteration[360/381] Loss: 0.0000 Acc:100.00%
Training: Epoch[034/035] Iteration[370/381] Loss: 0.0000 Acc:100.00%
Training: Epoch[034/035] Iteration[380/381] Loss: 0.0000 Acc:100.00%
Training: Epoch[035/035] Iteration[010/381] Loss: 0.0000 Acc:100.00%
Training: Epoch[035/035] Iteration[020/381] Loss: 0.0000 Acc:100.00%
Training: Epoch[035/035] Iteration[030/381] Loss: 0.0000 Acc:100.00%
Training: Epoch[035/035] Iteration[040/381] Loss: 0.0000 Acc:100.00%
Training: Epoch[035/035] Iteration[050/381] Loss: 0.0000 Acc:100.00%
Training: Epoch[035/035] Iteration[060/381] Loss: 0.0000 Acc:100.00%
Training: Epoch[035/035] Iteration[070/381] Loss: 0.0000 Acc:100.00%
Training: Epoch[035/035] Iteration[080/381] Loss: 0.0001 Acc:100.00%
Training: Epoch[035/035] Iteration[090/381] Loss: 0.0001 Acc:100.00%
Training: Epoch[035/035] Iteration[100/381] Loss: 0.0000 Acc:100.00%
Training: Epoch[035/035] Iteration[110/381] Loss: 0.0000 Acc:100.00%
Training: Epoch[035/035] Iteration[120/381] Loss: 0.0001 Acc:100.00%
Training: Epoch[035/035] Iteration[130/381] Loss: 0.0000 Acc:100.00%
Training: Epoch[035/035] Iteration[140/381] Loss: 0.0001 Acc:100.00%
Training: Epoch[035/035] Iteration[150/381] Loss: 0.0000 Acc:100.00%
Training: Epoch[035/035] Iteration[160/381] Loss: 0.0000 Acc:100.00%
Training: Epoch[035/035] Iteration[170/381] Loss: 0.0001 Acc:100.00%
Training: Epoch[035/035] Iteration[180/381] Loss: 0.0000 Acc:100.00%
Training: Epoch[035/035] Iteration[190/381] Loss: 0.0000 Acc:100.00%
Training: Epoch[035/035] Iteration[200/381] Loss: 0.0000 Acc:100.00%
Training: Epoch[035/035] Iteration[210/381] Loss: 0.0001 Acc:100.00%
Training: Epoch[035/035] Iteration[220/381] Loss: 0.0000 Acc:100.00%
Training: Epoch[035/035] Iteration[230/381] Loss: 0.0000 Acc:100.00%
Training: Epoch[035/035] Iteration[240/381] Loss: 0.0001 Acc:100.00%
Training: Epoch[035/035] Iteration[250/381] Loss: 0.0000 Acc:100.00%
Training: Epoch[035/035] Iteration[260/381] Loss: 0.0002 Acc:100.00%
Training: Epoch[035/035] Iteration[270/381] Loss: 0.0000 Acc:100.00%
Training: Epoch[035/035] Iteration[280/381] Loss: 0.0000 Acc:100.00%
Training: Epoch[035/035] Iteration[290/381] Loss: 0.0000 Acc:100.00%
Training: Epoch[035/035] Iteration[300/381] Loss: 0.0000 Acc:100.00%
Training: Epoch[035/035] Iteration[310/381] Loss: 0.0001 Acc:100.00%
Training: Epoch[035/035] Iteration[320/381] Loss: 0.0000 Acc:100.00%
Training: Epoch[035/035] Iteration[330/381] Loss: 0.0001 Acc:100.00%
Training: Epoch[035/035] Iteration[340/381] Loss: 0.0000 Acc:100.00%
Training: Epoch[035/035] Iteration[350/381] Loss: 0.0005 Acc:100.00%
Training: Epoch[035/035] Iteration[360/381] Loss: 0.0000 Acc:100.00%
Training: Epoch[035/035] Iteration[370/381] Loss: 0.0000 Acc:100.00%
Training: Epoch[035/035] Iteration[380/381] Loss: 0.0000 Acc:100.00%
Valid set Accuracy:90.68%
